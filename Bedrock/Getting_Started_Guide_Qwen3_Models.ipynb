{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Qwen Models Getting Started Guide on Amazon Bedrock\n",
        "\n",
        "This notebook provides a comprehensive introduction to using Qwen models on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for these advanced reasoning models. \n",
        "\n",
        "## Model Overview\n",
        "\n",
        "### Qwen-3-235B-A22B (MoE)\n",
        "\n",
        "**Parameters:** 235 billion total (Mixture-of-Experts hybrid model with 22B active per inference)\n",
        "\n",
        "**Use Cases:** Advanced reasoning tasks, agentic use cases, thinking and non-thinking modes\n",
        "\n",
        "**Key Features:**\n",
        "- **Thinking Mode**: Carefully works through problems step-by-step with enhanced reasoning\n",
        "- **Non-Thinking Mode**: Provides quick responses to straightforward questions\n",
        "- **Hybrid Architecture**: Mixture-of-Experts (MoE) design for optimal performance and compute efficiency\n",
        "- **Enhanced Tool Calling**: Superior performance in agent-based tasks\n",
        "- **Compute Efficient**: Only 22B parameters activated per inference despite 235B total parameters\n",
        "\n",
        "### Qwen3-32B (Dense)\n",
        "\n",
        "**Parameters:** 32 billion (Dense model with all parameters active)\n",
        "\n",
        "**Use Cases:** General-purpose tasks, instruction-following, conversational AI, enterprise applications\n",
        "\n",
        "**Key Features:**\n",
        "- **Consistent Performance**: All 32B parameters active during inference for robust performance\n",
        "- **Enterprise Ready**: Optimized for enterprise and research use cases\n",
        "- **Instruction Following**: Excellent at following complex instructions\n",
        "- **Conversational AI**: Strong conversational capabilities\n",
        "- **Tool Calling**: Enhanced function calling capabilities\n",
        "\n",
        "## Core Capabilities\n",
        "\n",
        "Both Qwen models offer the following characteristics:\n",
        "\n",
        "**Input/Output:** Text-in, text-out \n",
        "\n",
        "**Context Window:** 128,000 tokens  \n",
        "\n",
        "**Model Type:** Advanced reasoning models with thinking capabilities\n",
        "\n",
        "**Languages:** English and Chinese\n",
        "\n",
        "**Supported Regions:** \n",
        "- **US:** us-east-1 (N. Virginia), us-west-2 (Oregon)\n",
        "- **EU:** eu-west-1 (Ireland), eu-west-2 (London), eu-north-1 (Stockholm), eu-south-1 (Milan)\n",
        "- **AP:** ap-northeast-1 (Tokyo), ap-south-1 (Mumbai)\n",
        "- **SA:** sa-east-1 (São Paulo)\n",
        "\n",
        "**Tool Calling:** ✅ Supported (Enhanced capabilities)\n",
        "\n",
        "**Bedrock Guardrails** ✅ Supported\n",
        "\n",
        "**Converse API** ✅ Supported\n",
        "\n",
        "**OpenAI Chat Completions API** ✅ Supported\n",
        "\n",
        "**Streaming:** ✅ Supported\n",
        "\n",
        "**Model Evaluation:** ✅ Supported\n",
        "\n",
        "**Agents:** ✅ Supported\n",
        "\n",
        "**Prompt Management:** ✅ Supported\n",
        "\n",
        "**Flows:** ✅ Supported\n",
        "\n",
        "**Batch Inference:** ✅ Supported\n",
        "\n",
        "**Knowledge Bases:** ✅ Supported\n",
        "\n",
        "**Bedrock Studio:** ✅ Supported"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## What You'll Learn in this getting started guide\n",
        "\n",
        "- Options to use Amazon Bedrock for Qwen models inference, including:    \n",
        "    - Using the OpenAI SDK with Amazon Bedrock\n",
        "    - Using Amazon Bedrock's InvokeModel API\n",
        "    - Using Amazon Bedrock's Converse API\n",
        "- Understanding request parameters and response structures\n",
        "- Leveraging thinking vs non-thinking modes for different use cases\n",
        "- Implementing enhanced tool calling capabilities\n",
        "- Exploring reasoning capabilities with thinking mode\n",
        "- Comparing performance between Qwen-3-235B-A22B (MoE) and Qwen3-32B (Dense) models\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Access on Amazon Bedrock\n",
        "\n",
        "Ensure you have the correct IAM permission in order to access Qwen's models on Amazon Bedrock.\n",
        "\n",
        "## IAM Permissions\n",
        "\n",
        "To use Bedrock models, your AWS credentials need the following permissions:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "```json\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"bedrock:InvokeModel\",\n",
        "        \"bedrock:InvokeModelWithResponseStream\"\n",
        "      ],\n",
        "      \"Resource\": [\n",
        "        \"arn:aws:bedrock:*::foundation-model/qwen.qwen3-235b-a22b-2507-v1:0\",\n",
        "        \"arn:aws:bedrock:*::foundation-model/qwen.qwen3-32b-v1:0\"\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Note:** The wildcard (`*`) in the region field covers all supported regions:\n",
        "- **US:** us-east-1, us-west-2\n",
        "- **EU:** eu-west-1, eu-west-2, eu-north-1, eu-south-1  \n",
        "- **AP:** ap-northeast-1, ap-south-1\n",
        "- **SA:** sa-east-1\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Environment Configuration\n",
        "\n",
        "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
        "\n",
        "### Required Imports:\n",
        "- `os` → For environment variables\n",
        "- `boto3` → For native Bedrock API interactions  \n",
        "- `json` → For JSON serialization/deserialization\n",
        "- `datetime` → For timestamp tracking and performance measurements\n",
        "- `openai` → For OpenAI SDK compatibility with Bedrock\n",
        "- `strands` → For Amazon Strands agent framework\n",
        "- `IPython.display` → For enhanced output formatting and streaming demonstrations\n",
        "\n",
        "### Environment Variables:\n",
        "We set two environment variables to redirect the OpenAI SDK:\n",
        "- `AWS_BEARER_TOKEN_BEDROCK` → Your Bedrock API key  \n",
        "- `OPENAI_BASE_URL` → Bedrock's OpenAI-compatible endpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Usage:   \n",
            "  /opt/homebrew/anaconda3/bin/python -m pip <command> [options]\n",
            "\n",
            "no such option: -U\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip -U install boto3 openai ipython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output, display, display_markdown, Markdown\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Model IDs\n",
        "\n",
        "- **qwen.qwen3-235b-a22b-2507-v1:0** (MoE model with thinking mode)\n",
        "- **qwen.qwen3-32b-v1:0** (Dense model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Using model: deepseek.v3-v1:0\n"
          ]
        }
      ],
      "source": [
        "# Model Configuration - Qwen Models\n",
        "QWEN_MOE_MODEL_ID = \"qwen.qwen3-235b-a22b-2507-v1:0\"  # MoE model with thinking mode\n",
        "QWEN_DENSE_MODEL_ID = \"qwen.qwen3-32b-v1:0\"  # Dense model\n",
        "\n",
        "print(f\"✅ Using MoE model: {QWEN_MOE_MODEL_ID}\")\n",
        "print(f\"✅ Using Dense model: {QWEN_DENSE_MODEL_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Environment configured for Bedrock!\n"
          ]
        }
      ],
      "source": [
        "# Set environment variables to point to Bedrock\n",
        "# Note: Change the region in the URL to match your preferred region\n",
        "# Supported regions:\n",
        "# US: us-east-1, us-west-2\n",
        "# EU: eu-west-1, eu-west-2, eu-north-1, eu-south-1\n",
        "# AP: ap-northeast-1, ap-south-1\n",
        "# SA: sa-east-1\n",
        "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"<insert your bedrock API key>\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<insert your bedrock API key>\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
        "\n",
        "print(\"✅ Environment configured for Bedrock!\")\n",
        "print(\"📍 Using us-west-2 region - change the URL above to use a different region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Inference with Amazon Bedrock\n",
        "\n",
        "### Option 1: OpenAI SDK\n",
        "\n",
        "#### Import and Initialize OpenAI Client\n",
        "\n",
        "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
        "\n",
        "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ OpenAI client initialized (pointing to Bedrock)\n",
            "✅ Bedrock client initialized in region: us-west-2\n"
          ]
        }
      ],
      "source": [
        "# Initialize both clients\n",
        "# Note: Change region_name to match your preferred region\n",
        "# Supported regions:\n",
        "# US: us-east-1, us-west-2\n",
        "# EU: eu-west-1, eu-west-2, eu-north-1, eu-south-1\n",
        "# AP: ap-northeast-1, ap-south-1\n",
        "# SA: sa-east-1\n",
        "client = OpenAI()  # For chat completions API\n",
        "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')  \n",
        "\n",
        "print(\"✅ OpenAI client initialized (pointing to Bedrock)\")\n",
        "print(f\"✅ Bedrock client initialized in region: {bedrock_client.meta.region_name}\")\n",
        "print(\"📍 Change region_name above to use a different supported region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Make API Calls \n",
        "\n",
        "The API call structure is identical to OpenAI:\n",
        "- Same `messages` format with `role` and `content`\n",
        "- Same `model` parameter (but uses Bedrock model IDs)  \n",
        "- Same `stream` parameter for real-time responses\n",
        "- **New**: `thinking_mode` parameter to control thinking vs non-thinking behavior (available for MoE model)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "Completions.create() got an unexpected keyword argument 'thinking_mode'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[15], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example 1: Non-thinking mode (quick response)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m      3\u001b[0m     model\u001b[38;5;241m=\u001b[39mMODEL_ID,                 \n\u001b[1;32m      4\u001b[0m     messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m      5\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are a concise, highly logical assistant.\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m         {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m,   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat is the largest city in the southern hemisphere?\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      7\u001b[0m     ],\n\u001b[1;32m      8\u001b[0m     temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m      9\u001b[0m     max_completion_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m,\n\u001b[1;32m     10\u001b[0m     thinking_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Non-thinking mode for quick responses\u001b[39;00m\n\u001b[1;32m     11\u001b[0m )\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Extract and print the response text\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🤖 Non-thinking mode response:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.12/site-packages/openai/_utils/_utils.py:287\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 287\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
            "\u001b[0;31mTypeError\u001b[0m: Completions.create() got an unexpected keyword argument 'thinking_mode'"
          ]
        }
      ],
      "source": [
        "# Example 1: Qwen3-32B Dense model (quick response)\n",
        "response = client.chat.completions.create(\n",
        "    model=QWEN_DENSE_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=1000\n",
        ")\n",
        "\n",
        "# Extract and print the response text\n",
        "print(\"🤖 Qwen3-32B Dense model response:\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Qwen-3-235B-A22B MoE model with thinking mode (step-by-step reasoning)\n",
        "response = client.chat.completions.create(\n",
        "    model=QWEN_MOE_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"If a train leaves station A at 60 mph and another leaves station B at 40 mph, and they are 200 miles apart, when will they meet?\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=2000,\n",
        "    thinking_mode=True  # Thinking mode for complex reasoning (MoE model only)\n",
        ")\n",
        "\n",
        "# Extract and print the response text\n",
        "print(\"🧠 Qwen-3-235B-A22B MoE model with thinking mode response:\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 3: Qwen-3-235B-A22B MoE model without thinking mode (quick response)\n",
        "response = client.chat.completions.create(\n",
        "    model=QWEN_MOE_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"What is the capital of France?\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=1000,\n",
        "    thinking_mode=False  # Non-thinking mode for quick responses\n",
        ")\n",
        "\n",
        "# Extract and print the response text\n",
        "print(\"⚡ Qwen-3-235B-A22B MoE model without thinking mode response:\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Process Streaming Response\n",
        "\n",
        "Handle the response exactly like you would with OpenAI. Each `item` in the response is a chunk of the model's output. Both Qwen models support streaming, with the MoE model supporting streaming in both thinking and non-thinking modes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming with Qwen-3-235B-A22B MoE model thinking mode\n",
        "streaming_response = client.chat.completions.create(\n",
        "    model=QWEN_MOE_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"Explain how photosynthesis works in simple terms.\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=1500,\n",
        "    thinking_mode=True,  # Enable thinking mode\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Extract and print the response text in real-time.\n",
        "print(\"🧠 Streaming Qwen-3-235B-A22B MoE model with thinking mode response:\")\n",
        "for chunk in streaming_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming with Qwen3-32B Dense model\n",
        "streaming_response = client.chat.completions.create(\n",
        "    model=QWEN_DENSE_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"What are the benefits of renewable energy?\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=1500,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Extract and print the response text in real-time.\n",
        "print(\"🤖 Streaming Qwen3-32B Dense model response:\")\n",
        "for chunk in streaming_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### What's Happening Behind the Scenes?\n",
        "\n",
        "When you use the OpenAI SDK with Bedrock, your requests are automatically translated to Bedrock's native `InvokeModel` API.\n",
        "\n",
        "#### Request Translation\n",
        "- **OpenAI SDK Request** → **Bedrock InvokeModel** \n",
        "- The request body structure remains the same\n",
        "- But there are some key differences in how parameters are handled:\n",
        "\n",
        "| Parameter | OpenAI SDK | Bedrock InvokeModel |\n",
        "|-----------|------------|-------------------|\n",
        "| **Model ID** | In request body | Part of the URL path |\n",
        "| **Streaming** | `stream=True/False` | Different API endpoints:<br/>• `InvokeModel` (non-streaming)<br/>• `InvokeModelWithResponseStream` (streaming) |\n",
        "| **Request Body** | Full chat completions format | Same format, but `model` and `stream` are optional |\n",
        "| **Thinking Mode** | `thinking_mode=True/False` | Qwen-3-235B-A22B MoE model specific parameter |\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Enhanced Function Calling with OpenAI SDK\n",
        "\n",
        "Both Qwen models feature enhanced tool calling capabilities for superior performance in agent-based tasks. Let's demonstrate this with a weather lookup function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_weather(location):\n",
        "    \"\"\"\n",
        "    Get current weather for a given location.\n",
        "    This is a mock function that returns sample weather data.\n",
        "    \n",
        "    Args:\n",
        "        location (str): City and country, e.g. \"Paris, France\"\n",
        "        \n",
        "    Returns:\n",
        "        dict: Weather information\n",
        "    \"\"\"\n",
        "    # Mock weather data - in a real application, you'd call a weather API\n",
        "    weather_data = {\n",
        "        \"Paris, France\": {\"temperature\": \"22°C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
        "        \"New York, USA\": {\"temperature\": \"18°C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
        "        \"Tokyo, Japan\": {\"temperature\": \"25°C\", \"condition\": \"Rainy\", \"humidity\": \"80%\"},\n",
        "        \"London, UK\": {\"temperature\": \"15°C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"},\n",
        "        \"Sydney, Australia\": {\"temperature\": \"28°C\", \"condition\": \"Clear\", \"humidity\": \"55%\"}\n",
        "    }\n",
        "    \n",
        "    return weather_data.get(location, {\n",
        "        \"temperature\": \"20°C\", \n",
        "        \"condition\": \"Data not available\", \n",
        "        \"humidity\": \"50%\"\n",
        "    })\n",
        "\n",
        "# Define the function schema for OpenAI SDK\n",
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"get_weather\",\n",
        "        \"description\": \"Get current temperature and weather conditions for a given location.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"location\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"City and country, e.g. 'Paris, France'\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"location\"],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "}]\n",
        "\n",
        "print(\"✅ Weather function and tools configuration ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_functions(client, model, messages, tools, max_iterations=3, thinking_mode=False):\n",
        "    \"\"\"\n",
        "    Chat with function calling support using OpenAI SDK format.\n",
        "    \n",
        "    Args:\n",
        "        client: OpenAI client instance\n",
        "        model: Model ID to use\n",
        "        messages: List of conversation messages\n",
        "        tools: List of available tools/functions\n",
        "        max_iterations: Maximum number of function call iterations\n",
        "        thinking_mode: Whether to use thinking mode for enhanced reasoning (MoE model only)\n",
        "        \n",
        "    Returns:\n",
        "        Final assistant message\n",
        "    \"\"\"\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"🔄 Iteration {iteration + 1}\")\n",
        "        \n",
        "        # Make request with tools\n",
        "        request_params = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"tools\": tools,\n",
        "            \"tool_choice\": \"auto\"\n",
        "        }\n",
        "        \n",
        "        # Add thinking_mode only for MoE model\n",
        "        if thinking_mode and \"qwen3-235b-a22b\" in model:\n",
        "            request_params[\"thinking_mode\"] = thinking_mode\n",
        "        \n",
        "        response = client.chat.completions.create(**request_params)\n",
        "        \n",
        "        assistant_message = response.choices[0].message\n",
        "        messages.append(assistant_message)\n",
        "        \n",
        "        # Check if the model wants to call functions\n",
        "        if assistant_message.tool_calls:\n",
        "            print(f\"🔧 Model requested {len(assistant_message.tool_calls)} function call(s)\")\n",
        "            \n",
        "            # Process each function call\n",
        "            for tool_call in assistant_message.tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "                \n",
        "                print(f\"🔧 Calling function: {function_name}\")\n",
        "                print(f\"🔧 Arguments: {function_args}\")\n",
        "                \n",
        "                # Call the actual function\n",
        "                if function_name == \"get_weather\":\n",
        "                    function_result = get_weather(function_args[\"location\"])\n",
        "                    print(f\"🔧 Function result: {function_result}\")\n",
        "                else:\n",
        "                    function_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
        "                \n",
        "                # Add function result to conversation\n",
        "                function_message = {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"content\": json.dumps(function_result)\n",
        "                }\n",
        "                messages.append(function_message)\n",
        "                \n",
        "        else:\n",
        "            # No more function calls, return final response\n",
        "            print(\"✅ No function calls requested, conversation complete\")\n",
        "            return assistant_message\n",
        "    \n",
        "    print(\"⚠️ Maximum iterations reached\")\n",
        "    return assistant_message\n",
        "\n",
        "print(\"✅ Enhanced function calling handler ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test enhanced function calling with both models\n",
        "weather_questions = [\n",
        "    \"What's the weather like in Paris today?\",\n",
        "    \"Can you tell me the temperature in Tokyo?\",\n",
        "    \"How's the weather in Sydney, Australia?\",\n",
        "    \"What are the conditions like in New York?\"\n",
        "]\n",
        "\n",
        "print(\"🌤️ Testing Enhanced Function Calling with Qwen Models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test with Qwen3-32B Dense model\n",
        "print(\"\\n🤖 Testing with Qwen3-32B Dense Model\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "for i, question in enumerate(weather_questions[:2], 1):  # Test first 2 questions\n",
        "    print(f\"\\n📝 Test {i}: {question}\")\n",
        "    print(\"-\" * 20)\n",
        "    \n",
        "    try:\n",
        "        # Create conversation messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. Use the get_weather function to provide accurate weather information.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "        \n",
        "        # Call the function calling handler with dense model\n",
        "        final_response = chat_with_functions(\n",
        "            client=client,\n",
        "            model=QWEN_DENSE_MODEL_ID,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            thinking_mode=False  # Dense model doesn't support thinking mode\n",
        "        )\n",
        "        \n",
        "        # Print the final response\n",
        "        print(\"🤖 Final response:\")\n",
        "        print(final_response.content)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {str(e)}\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "# Test with Qwen-3-235B-A22B MoE model\n",
        "print(\"\\n🧠 Testing with Qwen-3-235B-A22B MoE Model (with thinking mode)\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, question in enumerate(weather_questions[2:], 1):  # Test last 2 questions\n",
        "    print(f\"\\n📝 Test {i}: {question}\")\n",
        "    print(\"-\" * 20)\n",
        "    \n",
        "    try:\n",
        "        # Create conversation messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. Use the get_weather function to provide accurate weather information.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "        \n",
        "        # Call the function calling handler with MoE model and thinking mode\n",
        "        final_response = chat_with_functions(\n",
        "            client=client,\n",
        "            model=QWEN_MOE_MODEL_ID,\n",
        "            messages=messages,\n",
        "            tools=tools,\n",
        "            thinking_mode=True  # Enable thinking mode for MoE model\n",
        "        )\n",
        "        \n",
        "        # Print the final response\n",
        "        print(\"🧠 Final response:\")\n",
        "        print(final_response.content)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"❌ Error: {str(e)}\")\n",
        "    \n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### What Just Happened with Enhanced Function Calling?\n",
        "\n",
        "The enhanced function calling demonstration shows both Qwen models' improved capabilities:\n",
        "\n",
        "1. **Enhanced Tool Recognition**: Both models have superior tool calling performance for agent-based tasks\n",
        "2. **Thinking Mode Integration**: The MoE model can reason through complex tool selection and usage when `thinking_mode=True`\n",
        "3. **Improved Function Execution**: Better understanding of when and how to use available tools\n",
        "4. **Multi-step Reasoning**: Both models can plan and execute complex multi-tool workflows\n",
        "5. **Context Awareness**: Enhanced understanding of conversation context for better tool usage decisions\n",
        "\n",
        "**Key Advantages of Qwen Models' Enhanced Tool Calling:**\n",
        "- More accurate tool selection based on user intent\n",
        "- Better handling of complex multi-step agent workflows  \n",
        "- Improved reasoning about tool parameters and results\n",
        "- Enhanced error handling and recovery in tool usage scenarios\n",
        "- **MoE Model**: Additional thinking mode for complex reasoning tasks\n",
        "- **Dense Model**: Consistent performance with all parameters active\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Option 2: Amazon Bedrock's InvokeModel API\n",
        "\n",
        "The Bedrock InvokeModel API is the foundational interface for interacting directly with any model hosted on Amazon Bedrock. It provides low-level, flexible access to model inference, allowing you to send input data and receive generated responses in a consistent way across all supported models.\n",
        "\n",
        "**Key Benefits:**\n",
        "- Direct Access: Interact with any Bedrock model using a unified API endpoint.\n",
        "- Fine-Grained Control: Customize inference parameters and payloads for each request.\n",
        "- Streaming Support: Use `InvokeModelWithResponseStream` for real-time, token-by-token output.\n",
        "- Privacy: Amazon Bedrock does not store your input or output data—requests are used only for inference.\n",
        "- **Thinking Mode Control**: Direct control over Qwen-3-235B-A22B MoE model's thinking vs non-thinking behavior.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Setup client\n",
        "\n",
        "First, we setup the Amazon Bedrock client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure region for Bedrock client\n",
        "# Supported regions:\n",
        "# US: us-east-1, us-west-2\n",
        "# EU: eu-west-1, eu-west-2, eu-north-1, eu-south-1\n",
        "# AP: ap-northeast-1, ap-south-1\n",
        "# SA: sa-east-1\n",
        "region = None\n",
        "\n",
        "if region is None:\n",
        "    target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
        "else:\n",
        "    target_region = \"us-west-2\"\n",
        "\n",
        "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)\n",
        "print(f\"📍 Using region: {target_region} - change the region variable above to use a different supported region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Inference with InvokeModel API\n",
        "\n",
        "Then we use the InvokeModel API to perform model inference with the two Qwen models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_model(body, model_id, accept, content_type):\n",
        "    \"\"\"\n",
        "    Invokes Amazon bedrock model to run an inference\n",
        "    using the input provided in the request body.\n",
        "    \n",
        "    Args:\n",
        "        body (dict): The invokation body to send to bedrock\n",
        "        model_id (str): the model to query\n",
        "        accept (str): input accept type\n",
        "        content_type (str): content type\n",
        "    Returns:\n",
        "        Inference response from the model.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = bedrock_runtime.invoke_model(\n",
        "            body=json.dumps(body), \n",
        "            modelId=model_id, \n",
        "            accept=accept, \n",
        "            contentType=content_type\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't invoke {model_id}\")\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with Qwen-3-235B-A22B MoE model and thinking mode\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 1000,\n",
        "    \"thinking_mode\": True  \n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "response = invoke_model(body, QWEN_MOE_MODEL_ID, accept, contentType)\n",
        "response_body = json.loads(response.get(\"body\").read())\n",
        "\n",
        "print(\"🧠 Thinking mode response:\")\n",
        "print(response_body['choices'][0]['message']['content'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with Qwen3-32B Dense model\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 1000,\n",
        "    \"thinking_mode\": True  \n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "response = invoke_model(body, QWEN_DENSE_MODEL_ID, accept, contentType)\n",
        "response_body = json.loads(response.get(\"body\").read())\n",
        "\n",
        "print(\"📝 Response:\")\n",
        "print(response_body['choices'][0]['message']['content'])"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Streaming with InvokeModel API\n",
        "\n",
        "The InvokeModel API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming with Qwen-3-235B-A22B MoE model and thinking mode\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 1000,\n",
        "    \"thinking_mode\": True  # Enable thinking mode for streaming\n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "response = bedrock_runtime.invoke_model_with_response_stream(\n",
        "    body=json.dumps(body), modelId=QWEN_MOE_MODEL_ID, accept=accept, contentType=contentType\n",
        ")\n",
        "chunk_count = 0\n",
        "time_to_first_token = None\n",
        "\n",
        "# Process the response stream\n",
        "stream = response.get(\"body\")\n",
        "if stream:\n",
        "    print(\"🧠 Streaming thinking mode response:\")\n",
        "    for event in stream:\n",
        "        chunk = event.get(\"chunk\")\n",
        "        if chunk:\n",
        "            # Print the response chunk\n",
        "            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
        "            content_block_delta = chunk_json.get(\"choices\")[0][\"delta\"].get(\"content\")\n",
        "            if content_block_delta:\n",
        "                if time_to_first_token is None:\n",
        "                    time_to_first_token = datetime.now() - start_time\n",
        "                    print(f\"Time to first token: {time_to_first_token}\")\n",
        "\n",
        "                chunk_count += 1\n",
        "                print(content_block_delta, end=\"\")\n",
        "    print(f\"\\nTotal chunks: {chunk_count}\")\n",
        "else:\n",
        "    print(\"No response stream received.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Option 3: Amazon Bedrock's Converse API\n",
        "\n",
        "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes. \n",
        "\n",
        "Key Benefits:\n",
        "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
        "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
        "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
        "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management\n",
        "- **Thinking Mode Support**: Direct control over Qwen-3-235B-A22B's thinking capabilities\n",
        "\n",
        "Additionally, the Converse API automatically separates the reasoning trace from the final response, giving developers the flexibility to show or hide the model's thinking process from end users based on their application needs.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converse API with Qwen3-32B Dense model (no thinking mode)\n",
        "response = bedrock_client.converse(\n",
        "    modelId=QWEN_DENSE_MODEL_ID,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"text\": \"What is the capital of Australia?\"}]\n",
        "        }\n",
        "    ],\n",
        "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
        "    inferenceConfig={\n",
        "        \"temperature\": 0,\n",
        "        \"maxTokens\": 1000\n",
        "    }\n",
        "    # Note: No additionalModelRequestFields needed for dense model\n",
        ")\n",
        "\n",
        "# Final response (dense model doesn't have reasoning trace)\n",
        "print(f\"🤖 Dense model response:\")\n",
        "print(response['output']['message']['content'][0]['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converse API with Qwen-3-235B-A22B MoE model and thinking mode\n",
        "response = bedrock_client.converse(\n",
        "    modelId=QWEN_MOE_MODEL_ID,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"text\": \"How far from earth is the moon?\"}]\n",
        "        }\n",
        "    ],\n",
        "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
        "    inferenceConfig={\n",
        "        \"temperature\": 0,\n",
        "        \"maxTokens\": 1000\n",
        "    },\n",
        "    additionalModelRequestFields={\n",
        "        \"thinking_mode\": True  # Enable thinking mode for MoE model\n",
        "    }\n",
        ")\n",
        "\n",
        "# Message dict\n",
        "print(f\"📝 Message dict:\")\n",
        "print(response['output']['message']['content'])\n",
        "\n",
        "# Reasoning trace (if available)\n",
        "if 'reasoningContent' in response['output']['message']['content'][0]:\n",
        "    print(f\"📝 Reasoning trace:\")\n",
        "    print(response['output']['message']['content'][0]['reasoningContent']['reasoningText']['text'])\n",
        "\n",
        "# Final response\n",
        "print(f\"📝 Final response:\")\n",
        "print(response['output']['message']['content'][1]['text'])"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Streaming with Converse API\n",
        "\n",
        "The Converse API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. The output below will contain reasoning trace and final response. Based on your application you might want to hide the reasoning trace from the end user."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming through Converse API with Qwen3-32B Dense model \n",
        "def bedrock_model_converse_stream_dense(client, system_prompt, user_prompt, max_tokens=1000, temperature=0):\n",
        "    response = \"\"\n",
        "    response = client.converse_stream(\n",
        "        modelId=QWEN_DENSE_MODEL_ID,\n",
        "        messages=[  \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"text\": user_prompt\n",
        "                    }\n",
        "                ]\n",
        "            },                        \n",
        "        ],\n",
        "        system=[{\"text\": system_prompt}],\n",
        "        inferenceConfig={\n",
        "            \"temperature\": temperature,\n",
        "            \"maxTokens\": max_tokens\n",
        "        }\n",
        "        # Note: No additionalModelRequestFields needed for dense model\n",
        "    )\n",
        "    # Extract and print the response text in real-time.\n",
        "    for event in response['stream']:\n",
        "        if 'contentBlockDelta' in event:\n",
        "            chunk = event['contentBlockDelta']\n",
        "            if chunk['delta'].get('text', None):\n",
        "                print(chunk['delta']['text'], end=\"\")\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming through Converse API with Qwen-3-235B-A22B MoE model and thinking mode\n",
        "\n",
        "def bedrock_model_converse_stream_moe(client, system_prompt, user_prompt, max_tokens=1000, temperature=0, thinking_mode=True):\n",
        "    response = \"\"\n",
        "    response = client.converse_stream(\n",
        "        modelId=QWEN_MOE_MODEL_ID,\n",
        "        messages=[  \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"text\": user_prompt\n",
        "                    }\n",
        "                ]\n",
        "            },                        \n",
        "        ],\n",
        "        system=[{\"text\": system_prompt}],\n",
        "        inferenceConfig={\n",
        "            \"temperature\": temperature,\n",
        "            \"maxTokens\": max_tokens\n",
        "        },\n",
        "        additionalModelRequestFields={\n",
        "            \"thinking_mode\": thinking_mode  # Enable thinking mode for Qwen-3-235B-A22B MoE model\n",
        "        }\n",
        "    )\n",
        "    # Extract and print the response text in real-time.\n",
        "    for event in response['stream']:\n",
        "        if 'contentBlockDelta' in event:\n",
        "            chunk = event['contentBlockDelta']\n",
        "            if chunk['delta'].get('reasoningContent', None):\n",
        "                print(chunk['delta']['reasoningContent']['text'], end=\"\")\n",
        "            if chunk['delta'].get('text', None):\n",
        "                print(chunk['delta']['text'], end=\"\")\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage of streaming functions\n",
        "\n",
        "print(\"\\n\\n🤖 Streaming with Qwen3-32B Dense model:\")\n",
        "bedrock_model_converse_stream_dense(\n",
        "    client=bedrock_client,\n",
        "    system_prompt=\"You are a helpful assistant.\",\n",
        "    user_prompt=\"What are the benefits of renewable energy?\"\n",
        ")\n",
        "\n",
        "print(\"🧠 Streaming with Qwen-3-235B-A22B MoE model (thinking mode):\")\n",
        "bedrock_model_converse_stream_moe(\n",
        "    client=bedrock_client,\n",
        "    system_prompt=\"You are a helpful assistant that thinks through problems step by step.\",\n",
        "    user_prompt=\"Explain how a computer works in simple terms.\",\n",
        "    thinking_mode=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Updated Conclusion for Qwen Models\n",
        "\n",
        "You've successfully explored **three powerful ways** to interact with Qwen models on Amazon Bedrock, including comprehensive tool use capabilities and thinking mode!\n",
        "\n",
        "### Model Comparison Summary\n",
        "\n",
        "| Feature | Qwen-3-235B-A22B (MoE) | Qwen3-32B (Dense) |\n",
        "|---------|------------------------|-------------------|\n",
        "| **Total Parameters** | 235B | 32B |\n",
        "| **Active Parameters** | 22B per inference | 32B (all active) |\n",
        "| **Thinking Mode** | ✅ Supported | ❌ Not supported |\n",
        "| **Use Cases** | Advanced reasoning, complex tasks | General-purpose, enterprise |\n",
        "| **Compute Efficiency** | High (only 22B active) | Consistent (all 32B active) |\n",
        "| **Performance** | Optimized for complex reasoning | Optimized for consistent performance |\n",
        "\n",
        "### Key Benefits Achieved\n",
        "\n",
        "✅ **Flexibility**: Three different API approaches for different use cases  \n",
        "✅ **Performance**: Streaming support for improved user experience  \n",
        "✅ **Familiarity**: Use existing OpenAI SDK patterns with AWS infrastructure  \n",
        "✅ **Control**: Direct API access when you need fine-grained customization  \n",
        "✅ **Consistency**: Universal interface that works across all Bedrock models  \n",
        "✅ **Privacy**: AWS Bedrock doesn't store your data - only used for inference  \n",
        "✅ **Tool Integration**: Enhanced function calling capabilities across all three approaches\n",
        "✅ **Practical Comparison**: Side-by-side examples using the same function\n",
        "✅ **Thinking Mode**: Step-by-step reasoning capabilities for complex problems (MoE model)\n",
        "✅ **Hybrid Architecture**: Mixture-of-Experts design for optimal performance and compute efficiency\n",
        "✅ **Model Choice**: Both MoE and Dense architectures for different use cases\n",
        "✅ **Cross-Region Inference**: Support for multiple AWS regions\n",
        "✅ **Knowledge Bases**: Integration with Amazon Bedrock Knowledge Bases\n",
        "✅ **Bedrock Studio**: Full integration with Bedrock Studio for development\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "You're now equipped with comprehensive knowledge to choose the right API approach and model for your specific use case. Whether you need:\n",
        "- The **simplicity** of the OpenAI SDK\n",
        "- The **control** of InvokeModel \n",
        "- The **consistency** of Converse API\n",
        "- **Enhanced tool use capabilities** for external integrations\n",
        "- **Thinking mode** for complex reasoning tasks (MoE model)\n",
        "\n",
        "You have all the tools and examples to build powerful AI applications with Qwen models' advanced capabilities on Amazon Bedrock!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
