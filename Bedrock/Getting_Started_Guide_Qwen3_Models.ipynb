{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen Models Getting Started Guide on Amazon Bedrock\n",
    "\n",
    "This notebook provides a comprehensive introduction to using Qwen models on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for these advanced reasoning models. \n",
    "\n",
    "## Model Overview\n",
    "\n",
    "### Qwen-3-235B-A22B (MoE)\n",
    "\n",
    "**Parameters:** 235 billion total (Mixture-of-Experts model with 22B active per inference)\n",
    "\n",
    "**Use Cases:** Advanced reasoning tasks, agentic use cases, instruction-following\n",
    "\n",
    "**Key Features:**\n",
    "- **Instruct/Non-Thinking Model**: Provides direct responses optimized for instruction-following\n",
    "- **Architecture**: Mixture-of-Experts (MoE) design for optimal performance and compute efficiency\n",
    "- **Enhanced Tool Calling**: Superior performance in agent-based tasks\n",
    "- **Compute Efficient**: Only 22B parameters activated per inference despite 235B total parameters\n",
    "- **Enterprise Ready**: Optimized for complex enterprise applications\n",
    "\n",
    "### Qwen3-32B (Dense)\n",
    "\n",
    "**Parameters:** 32 billion (Dense model with all parameters active)\n",
    "\n",
    "**Use Cases:** General-purpose tasks, reasoning problems, conversational AI, enterprise applications\n",
    "\n",
    "**Key Features:**\n",
    "- **Hybrid Thinking Model**: Can operate in both thinking and non-thinking modes\n",
    "- **Thinking Mode**: Carefully works through problems step-by-step with enhanced reasoning\n",
    "- **Non-Thinking Mode**: Provides quick responses to straightforward questions\n",
    "- **Consistent Performance**: All 32B parameters active during inference for robust performance\n",
    "- **Instruction Following**: Excellent at following complex instructions\n",
    "- **Conversational AI**: Strong conversational capabilities\n",
    "- **Tool Calling**: Enhanced function calling capabilities\n",
    "\n",
    "## Core Capabilities\n",
    "\n",
    "Both Qwen models offer the following characteristics:\n",
    "\n",
    "**Input/Output:** Text-in, text-out \n",
    "\n",
    "**Context Window:** 128,000 tokens  \n",
    "\n",
    "**Model Type:** Advanced reasoning models with thinking capabilities\n",
    "\n",
    "**Languages:** English and Chinese\n",
    "\n",
    "**Supported Regions:** see [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html)\n",
    "\n",
    "**Tool Calling:** ✅ Supported (Enhanced capabilities)\n",
    "\n",
    "**Bedrock Guardrails** ✅ Supported\n",
    "\n",
    "**Converse API** ✅ Supported\n",
    "\n",
    "**OpenAI Chat Completions API** ✅ Supported\n",
    "\n",
    "**Streaming:** ✅ Supported\n",
    "\n",
    "**Model Evaluation:** ✅ Supported\n",
    "\n",
    "**Agents:** ✅ Supported\n",
    "\n",
    "**Prompt Management:** ✅ Supported\n",
    "\n",
    "**Flows:** ✅ Supported\n",
    "\n",
    "**Batch Inference:** ✅ Supported\n",
    "\n",
    "**Knowledge Bases:** ✅ Supported\n",
    "\n",
    "**Bedrock Studio:** ✅ Supported"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## What You'll Learn in this getting started guide\n",
    "\n",
    "- Options to use Amazon Bedrock for Qwen models inference, including:    \n",
    "    - Using the OpenAI SDK with Amazon Bedrock\n",
    "    - Using Amazon Bedrock's InvokeModel API\n",
    "    - Using Amazon Bedrock's Converse API\n",
    "- Understanding request parameters and response structures\n",
    "- Leveraging thinking vs non-thinking modes for different use cases\n",
    "- Implementing enhanced tool calling capabilities\n",
    "- Exploring reasoning capabilities with thinking mode\n",
    "- Comparing performance between Qwen-3-235B-A22B (MoE) and Qwen3-32B (Dense) models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Model Access on Amazon Bedrock\n",
    "\n",
    "Ensure you have the correct IAM permission in order to access Qwen's models on Amazon Bedrock.\n",
    "\n",
    "## IAM Permissions\n",
    "\n",
    "To use Bedrock models, your AWS credentials need the following permissions:\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "```json\n",
    "{\n",
    "  \"Version\": \"2012-10-17\",\n",
    "  \"Statement\": [\n",
    "    {\n",
    "      \"Effect\": \"Allow\",\n",
    "      \"Action\": [\n",
    "        \"bedrock:InvokeModel\",\n",
    "        \"bedrock:InvokeModelWithResponseStream\"\n",
    "      ],\n",
    "      \"Resource\": [\n",
    "        \"arn:aws:bedrock:*::foundation-model/qwen.qwen3-235b-a22b-2507-v1:0\",\n",
    "        \"arn:aws:bedrock:*::foundation-model/qwen.qwen3-32b-v1:0\"\n",
    "      ]\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Note:** The wildcard (`*`) in the region field covers all supported regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 1: Environment Configuration\n",
    "\n",
    "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
    "\n",
    "### Required Imports:\n",
    "- `os` → For environment variables\n",
    "- `boto3` → For native Bedrock API interactions  \n",
    "- `json` → For JSON serialization/deserialization\n",
    "- `datetime` → For timestamp tracking and performance measurements\n",
    "- `openai` → For OpenAI SDK compatibility with Bedrock\n",
    "- `strands` → For Amazon Strands agent framework\n",
    "- `IPython.display` → For enhanced output formatting and streaming demonstrations\n",
    "\n",
    "### Environment Variables:\n",
    "We set two environment variables to redirect the OpenAI SDK:\n",
    "- `AWS_BEARER_TOKEN_BEDROCK` → Your Bedrock API key  \n",
    "- `OPENAI_BASE_URL` → Bedrock's OpenAI-compatible endpoint\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting boto3\n",
      "  Downloading boto3-1.40.38-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting openai\n",
      "  Downloading openai-1.109.1-py3-none-any.whl.metadata (29 kB)\n",
      "Requirement already satisfied: ipython in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (9.5.0)\n",
      "Collecting botocore<1.41.0,>=1.40.38 (from boto3)\n",
      "  Downloading botocore-1.40.38-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.15.0,>=0.14.0 (from boto3)\n",
      "  Using cached s3transfer-0.14.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from botocore<1.41.0,>=1.40.38->boto3) (2.9.0.post0)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.41.0,>=1.40.38->boto3)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.38->boto3) (1.17.0)\n",
      "Collecting anyio<5,>=3.5.0 (from openai)\n",
      "  Downloading anyio-4.11.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting distro<2,>=1.7.0 (from openai)\n",
      "  Using cached distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpx<1,>=0.23.0 (from openai)\n",
      "  Using cached httpx-0.28.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting jiter<1,>=0.4.0 (from openai)\n",
      "  Using cached jiter-0.11.0-cp313-cp313-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Collecting pydantic<3,>=1.9.0 (from openai)\n",
      "  Using cached pydantic-2.11.9-py3-none-any.whl.metadata (68 kB)\n",
      "Collecting sniffio (from openai)\n",
      "  Using cached sniffio-1.3.1-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting tqdm>4 (from openai)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting typing-extensions<5,>=4.11 (from openai)\n",
      "  Using cached typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting idna>=2.8 (from anyio<5,>=3.5.0->openai)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached certifi-2025.8.3-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting httpcore==1.* (from httpx<1,>=0.23.0->openai)\n",
      "  Using cached httpcore-1.0.9-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting h11>=0.16 (from httpcore==1.*->httpx<1,>=0.23.0->openai)\n",
      "  Using cached h11-0.16.0-py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.33.2 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl.metadata (6.8 kB)\n",
      "Collecting typing-inspection>=0.4.0 (from pydantic<3,>=1.9.0->openai)\n",
      "  Using cached typing_inspection-0.4.1-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: decorator in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (4.9.0)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (2.19.2)\n",
      "Requirement already satisfied: stack_data in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from ipython) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from prompt_toolkit<3.1.0,>=3.0.41->ipython) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from jedi>=0.16->ipython) (0.8.5)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from pexpect>4.3->ipython) (0.7.0)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from stack_data->ipython) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from stack_data->ipython) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in /Users/jeffzeng/Documents/Projects/sample-qwen-on-aws/.venv/lib/python3.13/site-packages (from stack_data->ipython) (0.2.3)\n",
      "Downloading boto3-1.40.38-py3-none-any.whl (139 kB)\n",
      "Downloading botocore-1.40.38-py3-none-any.whl (14.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.0/14.0 MB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached s3transfer-0.14.0-py3-none-any.whl (85 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading openai-1.109.1-py3-none-any.whl (948 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m948.6/948.6 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading anyio-4.11.0-py3-none-any.whl (109 kB)\n",
      "Using cached distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "Using cached httpcore-1.0.9-py3-none-any.whl (78 kB)\n",
      "Using cached jiter-0.11.0-cp313-cp313-macosx_11_0_arm64.whl (314 kB)\n",
      "Using cached pydantic-2.11.9-py3-none-any.whl (444 kB)\n",
      "Using cached pydantic_core-2.33.2-cp313-cp313-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached h11-0.16.0-py3-none-any.whl (37 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached sniffio-1.3.1-py3-none-any.whl (10 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Using cached typing_inspection-0.4.1-py3-none-any.whl (14 kB)\n",
      "Using cached certifi-2025.8.3-py3-none-any.whl (161 kB)\n",
      "Installing collected packages: urllib3, typing-extensions, tqdm, sniffio, jmespath, jiter, idna, h11, distro, certifi, annotated-types, typing-inspection, pydantic-core, httpcore, botocore, anyio, s3transfer, pydantic, httpx, openai, boto3\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21/21\u001b[0m [boto3]m20/21\u001b[0m [boto3]]c]\n",
      "\u001b[1A\u001b[2KSuccessfully installed annotated-types-0.7.0 anyio-4.11.0 boto3-1.40.38 botocore-1.40.38 certifi-2025.8.3 distro-1.9.0 h11-0.16.0 httpcore-1.0.9 httpx-0.28.1 idna-3.10 jiter-0.11.0 jmespath-1.0.1 openai-1.109.1 pydantic-2.11.9 pydantic-core-2.33.2 s3transfer-0.14.0 sniffio-1.3.1 tqdm-4.67.1 typing-extensions-4.15.0 typing-inspection-0.4.1 urllib3-2.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install boto3 openai ipython\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import json\n",
    "from openai import OpenAI\n",
    "from datetime import datetime\n",
    "from IPython.display import clear_output, display, display_markdown, Markdown\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Model IDs\n",
    "\n",
    "- **qwen.qwen3-235b-a22b-2507-v1:0** (MoE model with thinking mode)\n",
    "- **qwen.qwen3-32b-v1:0** (Dense model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using MoE model: qwen.qwen3-235b-a22b-2507-v1:0\n",
      "✅ Using Dense model: qwen.qwen3-32b-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Model Configuration - Qwen Models\n",
    "QWEN_MOE_MODEL_ID = \"qwen.qwen3-235b-a22b-2507-v1:0\"  # MoE model with thinking mode\n",
    "QWEN_DENSE_MODEL_ID = \"qwen.qwen3-32b-v1:0\"  # Dense model\n",
    "\n",
    "print(f\"✅ Using MoE model: {QWEN_MOE_MODEL_ID}\")\n",
    "print(f\"✅ Using Dense model: {QWEN_DENSE_MODEL_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Environment configured for Bedrock!\n",
      "📍 Using us-west-2 region - change the URL above to use a different region\n"
     ]
    }
   ],
   "source": [
    "# Set environment variables to point to Bedrock\n",
    "# Note: Change the region in the URL to match your preferred region\n",
    "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"ABSKQmVkcm9ja0FQSUtleS0xZ2N2LWF0LTQwNTY0NTIyMjcyODp1dnBlN1RCT2VCRHJNbFJ2UE0yWFFnWW9ZSHpocUk1T2RTa0dVV01CcTNGYTQyNGJCZFlpNnBIempGcz0=\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"ABSKQmVkcm9ja0FQSUtleS0xZ2N2LWF0LTQwNTY0NTIyMjcyODp1dnBlN1RCT2VCRHJNbFJ2UE0yWFFnWW9ZSHpocUk1T2RTa0dVV01CcTNGYTQyNGJCZFlpNnBIempGcz0=\"\n",
    "# os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"<insert your bedrock API key>\"\n",
    "# os.environ[\"OPENAI_API_KEY\"] = \"<insert your bedrock API key>\"\n",
    "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
    "\n",
    "print(\"✅ Environment configured for Bedrock!\")\n",
    "print(\"📍 Using us-west-2 region - change the URL above to use a different region\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Step 2: Inference with Amazon Bedrock\n",
    "\n",
    "### Option 1: OpenAI SDK\n",
    "\n",
    "#### Import and Initialize OpenAI Client\n",
    "\n",
    "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
    "\n",
    "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ OpenAI client initialized (pointing to Bedrock)\n",
      "✅ Bedrock client initialized in region: us-west-2\n",
      "📍 Change region_name above to use a different supported region\n"
     ]
    }
   ],
   "source": [
    "# Initialize both clients\n",
    "# Note: Change region_name to match your preferred region\n",
    "client = OpenAI()  # For chat completions API\n",
    "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')  \n",
    "\n",
    "print(\"✅ OpenAI client initialized (pointing to Bedrock)\")\n",
    "print(f\"✅ Bedrock client initialized in region: {bedrock_client.meta.region_name}\")\n",
    "print(\"📍 Change region_name above to use a different supported region\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Make API Calls \n",
    "\n",
    "The API call structure is identical to OpenAI:\n",
    "- Same `messages` format with `role` and `content`\n",
    "- Same `model` parameter (but uses Bedrock model IDs)  \n",
    "- Same `stream` parameter for real-time responses\n",
    "- **New**: `reasoning_effort` parameter to control thinking vs non-thinking behavior (available for MoE model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Qwen3-32B Dense model response:\n",
      "The largest city in the southern hemisphere is **São Paulo**, Brazil, by population.\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Qwen3-32B Dense model (quick response)\n",
    "response = client.chat.completions.create(\n",
    "    model=QWEN_DENSE_MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=1000,\n",
    "    \n",
    ")\n",
    "\n",
    "# Extract and print the response text\n",
    "print(\"🤖 Qwen3-32B Dense model response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Qwen3-32B Dense model with thinking mode response:\n",
      "<reasoning>\n",
      "Okay, so there's this problem about two trains leaving stations A and B, which are 200 miles apart. One is going at 60 mph and the other at 40 mph. The question is when they'll meet. Hmm, let me think.\n",
      "\n",
      "First, I need to visualize the scenario. Let me imagine two stations, A and B, 200 miles apart. A train leaves A heading towards B at 60 mph, and another leaves B heading towards A at 40 mph. They're moving towards each other, right? So their speeds are adding up because they're approaching each other. That makes sense because if two objects move towards each other, their relative speed is the sum of their individual speeds.\n",
      "\n",
      "So, if I add 60 mph and 40 mph, that gives me 100 mph. This combined speed means that the distance between them is decreasing at a rate of 100 miles per hour. Since they start 200 miles apart, I can figure out the time it takes for them to meet by dividing the total distance by their combined speed.\n",
      "\n",
      "Let me write that down. Time equals distance divided by speed. So time = 200 miles / 100 mph. That would be 2 hours. Wait, is that right? Let me check.\n",
      "\n",
      "If the first train is going 60 mph, in 2 hours it would have traveled 60 * 2 = 120 miles. The second train is going 40 mph, so in 2 hours it would have gone 40 * 2 = 80 miles. Adding those distances together, 120 + 80 = 200 miles, which matches the initial distance between the stations. So that checks out. They meet after 2 hours.\n",
      "\n",
      "But wait, let me make sure I didn't miss anything. The problem says \"if a train leaves station A at 60 mph and another leaves station B at 40 mph.\" It doesn't specify the direction, but since they are 200 miles apart, I assume they are moving towards each other. If they were moving in the same direction, the problem would be different. For example, if both were going from A to B, then the one from A would be catching up to the one from B, but since the one from B is going towards A, that's not the case here. The problem probably implies they are moving towards each other.\n",
      "\n",
      "Another way to think about it is using equations. Let's denote the time until they meet as t hours. In that time, the first train would have covered a distance of 60t miles, and the second train would have covered 40t miles. Since they started 200 miles apart, the sum of the distances they travel should equal 200 miles. So:\n",
      "\n",
      "60t + 40t = 200\n",
      "\n",
      "Combining like terms:\n",
      "\n",
      "100t = 200\n",
      "\n",
      "Divide both sides by 100:\n",
      "\n",
      "t = 2\n",
      "\n",
      "Yep, same answer. So that seems solid. I don't think there's any trick here. Maybe if there was a different starting time or something, but the problem states they both leave their respective stations, presumably at the same time? The problem doesn't specify different departure times, so I assume they start at the same moment.\n",
      "\n",
      "If they didn't start at the same time, we would need more information. But since it's not mentioned, same departure time is the standard assumption. So with that, the answer is 2 hours.\n",
      "\n",
      "Just to recap: combined speed is 60 + 40 = 100 mph, distance is 200 miles, time is 200 / 100 = 2 hours. Check with individual distances: 60*2=120, 40*2=80, 120+80=200. All adds up. I think that's correct. No need to overcomplicate it. The answer is 2 hours after they both depart.\n",
      "</reasoning> two trains will meet after **2 hours**. \n",
      "\n",
      "**Step-by-Step Explanation:**\n",
      "\n",
      "1. **Understand the Scenario:**\n",
      "   - Train A departs Station A at 60 mph.\n",
      "   - Train B departs Station B at 40 mph.\n",
      "   - The stations are 200 miles apart, and the trains are moving toward each other.\n",
      "\n",
      "2. **Calculate Combined Speed:**\n",
      "   - When two objects move toward each other, their relative speed is the sum of their individual speeds.\n",
      "   - Combined speed = 60 mph + 40 mph = **100 mph**.\n",
      "\n",
      "3. **Determine Time to Meet:**\n",
      "   - Time = Distance / Speed.\n",
      "   - Time = 200 miles / 100 mph = **2 hours**.\n",
      "\n",
      "4. **Verify with Individual Distances:**\n",
      "   - In 2 hours, Train A travels: 60 mph × 2 h = **120 miles**.\n",
      "   - In 2 hours, Train B travels: 40 mph × 2 h = **80 miles**.\n",
      "   - Total distance covered: 120 + 80 = **200 miles**, confirming they meet.\n",
      "\n",
      "**Answer:** The trains will meet **2 hours** after departure.\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Qwen3-32B Dense model with thinking mode (step-by-step reasoning)\n",
    "response = client.chat.completions.create(\n",
    "    model=QWEN_DENSE_MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"If a train leaves station A at 60 mph and another leaves station B at 40 mph, and they are 200 miles apart, when will they meet?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=2000,\n",
    "    reasoning_effort='high'# Thinking mode for complex reasoning (MoE model only)\n",
    ")\n",
    "\n",
    "# Extract and print the response text\n",
    "print(\"🧠 Qwen3-32B Dense model with thinking mode response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚡ Qwen-3-235B-A22B MoE model without thinking mode response:\n",
      "The capital of France is Paris.\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Qwen-3-235B-A22B MoE model without thinking mode (quick response)\n",
    "response = client.chat.completions.create(\n",
    "    model=QWEN_MOE_MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"What is the capital of France?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=1000,  # Non-thinking mode for quick responses\n",
    ")\n",
    "\n",
    "# Extract and print the response text\n",
    "print(\"⚡ Qwen-3-235B-A22B MoE model without thinking mode response:\")\n",
    "print(response.choices[0].message.content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Process Streaming Response\n",
    "\n",
    "Handle the response exactly like you would with OpenAI. Each `item` in the response is a chunk of the model's output. Both Qwen models support streaming, with the MoE model supporting streaming in both thinking and non-thinking modes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Streaming Qwen3-32B Dense model with thinking mode response:\n",
      "<reasoning>\n",
      "Okay, so I need to explain how photosynthesis works in simple terms. Let me start by recalling what I know about photosynthesis. I remember that it's a process plants use to make their own food. But I'm a bit fuzzy on the details. Let me think.\n",
      "\n",
      "First, plants take in something from the air</reasoning><reasoning>. I think it's carbon dioxide. Then they use sunlight, right? So sunlight is a key part of this process. They also need water, which they probably get from the soil through their roots. So water</reasoning><reasoning> comes up from the roots, and carbon dioxide comes in through the leaves, maybe through tiny holes called stomata?\n",
      "\n",
      "Then there's the part about chlorophyll. I remember that's the green pigment in plants that helps them absorb sunlight. So</reasoning><reasoning> chlorophyll is in the chloroplasts of the plant cells. The sunlight energy is used to convert the carbon dioxide and water into something else. The products are glucose and oxygen. Glucose is a</reasoning><reasoning> type of sugar that the plant uses for energy and growth. The oxygen is released into the air as a byproduct.\n",
      "\n",
      "Wait, so the basic equation is carbon dioxide plus water, with sunlight, making glucose and oxygen. I think the formula is 6CO2 + 6H2O + light → C6H1</reasoning><reasoning>2O6 + 6O2. But I need to explain this in simple terms without getting too technical.\n",
      "\n",
      "Let me break it down step by step. The plant takes in carbon dioxide from the air and water from the soil. Using sunlight, which is captured by chlorophyll, the plant combines these to make glucose (food) and releases</reasoning><reasoning> oxygen. The glucose is used for energy and to build parts of the plant. The oxygen is what we breathe in, so it's important for animals and humans.\n",
      "\n",
      "I should also mention where this happens in the plant. The</reasoning><reasoning> process mainly occurs in the leaves, specifically in the cells that have chloroplasts. The leaves are broad and flat to maximize sunlight absorption. The stomata on the underside of the leaves allow carbon</reasoning><reasoning> dioxide to enter and oxygen to exit.\n",
      "\n",
      "Wait, but I should make sure I'm not mixing up any steps. Let me check if I have the inputs and outputs right. Carbon dioxide and water in, glucose and oxygen out. Yes, that's correct</reasoning><reasoning>. The glucose is stored or used immediately. The oxygen is released through the stomata.\n",
      "\n",
      "I should also explain why this is important. Photosynthesis is how plants produce their own food and also how they release</reasoning><reasoning> oxygen into the atmosphere, which is essential for life on Earth. It's the basis of the food chain because other organisms depend on plants for food, either directly or indirectly.\n",
      "\n",
      "Hmm, maybe I should avoid using too many scientific terms. Instead of \"</reasoning><reasoning>chlorophyll,\" maybe say \"special green substance\" or something. But I think \"chlorophyll\" is a key term here. Maybe explain it as the green pigment that helps plants</reasoning><reasoning> capture sunlight.\n",
      "\n",
      "Also, the process has two main stages: the light-dependent reactions and the Calvin cycle (light-independent reactions). But in simple terms, maybe just say that the plant uses sunlight to split water into oxygen and hydrogen, and then uses the carbon dioxide to make glucose.</reasoning><reasoning> But I need to keep it simple without getting into the two stages.\n",
      "\n",
      "Let me try to put it all together in a simple explanation. Start with the basics: plants make their own food using sunlight, water, and carbon dioxide. They take in water through their roots and carbon dioxide through their leaves. Using sunlight (</reasoning><reasoning>captured by chlorophyll), they convert these into glucose (food) and release oxygen. The glucose is used for energy and growth, and the oxygen is released into the air.\n",
      "\n",
      "I should also mention that this process is vital for the ecosystem because it provides oxygen and forms the base of the food chain. Without photosynthesis, most life on Earth wouldn't exist.\n",
      "\n",
      "</reasoning><reasoning>Wait, but I need to make sure I'm not missing any key points. Let me verify the inputs and outputs again. Water (from roots), carbon dioxide (from air), sunlight (energy source) → glucose (food) and oxygen (released). Yes, that's right.\n",
      "\n",
      "</reasoning><reasoning>I think that's a solid outline. Now, I need to present this in a clear, simple way without jargon. Use everyday language and maybe an analogy if possible. For example, comparing the process to a factory where the plant is the</reasoning><reasoning> factory that uses sunlight as power to make food from raw materials (water and carbon dioxide).\n",
      "</reasoning> is how plants make their own food using sunlight, water, and air. Here's a simple breakdown:\n",
      "\n",
      "1. **Ingredients**:  \n",
      "   - **Water** is absorbed by the plant's roots from the soil.  \n",
      "   - **Carbon dioxide** is taken in through tiny holes in the leaves (called stomata).  \n",
      "   - **Sunlight** is captured by a green pigment in the leaves called **chlorophyll**.\n",
      "\n",
      "2. **The Process**:  \n",
      "   - Using sunlight as energy, the plant combines water and carbon dioxide.  \n",
      "   - This creates **glucose** (a type of sugar, like plant \"food\") and **oxygen**.\n",
      "\n",
      "3. **What Happens Next**:  \n",
      "   - **Glucose** is used by the plant for energy and to grow (like building blocks for leaves, stems, and roots).  \n",
      "   - **Oxygen** is released into the air through the leaves, which animals and humans need to breathe.\n",
      "\n",
      "**Why It Matters**:  \n",
      "- Photosynthesis is the foundation of life on Earth. It provides oxygen for us to breathe and forms the base of the food chain (plants feed animals, which feed other animals).  \n",
      "- Without it, most life wouldn’t exist!\n",
      "\n",
      "**Analogy**: Think of a plant as a tiny factory. The sunlight is the power source, and the water and carbon dioxide are the raw materials. The factory produces food (glucose) and releases oxygen as a \"waste product.\" 🌱☀️💨"
     ]
    }
   ],
   "source": [
    "# Streaming with Qwen3-32B Dense model thinking mode\n",
    "streaming_response = client.chat.completions.create(\n",
    "    model=QWEN_DENSE_MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant that thinks through problems step by step.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"Explain how photosynthesis works in simple terms.\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=1500,\n",
    "    reasoning_effort='high',  # Enable thinking mode\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "print(\"🧠 Streaming Qwen3-32B Dense model with thinking mode response:\")\n",
    "for chunk in streaming_response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 Streaming Qwen-3-235B-A22B MoE model response:\n",
      "Renewable energy offers numerous benefits for the environment, economy, and society. Here are some of the key advantages:\n",
      "\n",
      "1. **Environmental Protection**:\n",
      "   - **Reduces Greenhouse Gas Emissions**: Renewable sources like solar, wind, and hydropower produce little to no carbon dioxide or other greenhouse gases, helping to combat climate change.\n",
      "   - **Improves Air Quality**: Unlike fossil fuels, renewables don’t emit harmful pollutants such as sulfur dioxide, nitrogen oxides, or particulate matter, leading to cleaner air and better public health.\n",
      "   - **Conserves Water Resources**: Most renewable energy technologies use significantly less water than fossil fuel power plants, especially important in water-scarce regions.\n",
      "\n",
      "2. **Sustainability and Energy Security**:\n",
      "   - **Inexhaustible Supply**: Renewable energy sources like sunlight, wind, and water are naturally replenished and won’t run out, unlike finite fossil fuels.\n",
      "   - **Reduces Dependence on Imported Fuels**: By using locally available renewable resources, countries can enhance energy independence and reduce vulnerability to global fuel price fluctuations.\n",
      "\n",
      "3. **Economic Benefits**:\n",
      "   - **Job Creation**: The renewable energy sector creates jobs in manufacturing, installation, maintenance, and research, often in rural or underserved areas.\n",
      "   - **Stable Energy Prices**: Renewables have low operating costs and are less susceptible to price volatility compared to fossil fuels.\n",
      "   - **Stimulates Innovation and Investment**: Growth in renewables drives technological advancements and attracts investment in clean energy infrastructure.\n",
      "\n",
      "4. **Public Health Improvements**:\n",
      "   - Lower air and water pollution from renewable energy reduces respiratory illnesses, heart disease, and other health problems, leading to lower healthcare costs and improved quality of life.\n",
      "\n",
      "5. **Energy Access and Equity**:\n",
      "   - Renewables can provide electricity to remote or off-grid communities through decentralized systems like solar home systems or microgrids, improving energy access and supporting development.\n",
      "\n",
      "6. **Climate Resilience**:\n",
      "   - Diversifying the energy mix with renewables helps build a more resilient energy system that can better withstand extreme weather events and other climate-related disruptions.\n",
      "\n",
      "Overall, transitioning to renewable energy is a key step toward a cleaner, healthier, and more sustainable future."
     ]
    }
   ],
   "source": [
    "# Streaming with Qwen-3-235B-A22B MoE model\n",
    "streaming_response = client.chat.completions.create(\n",
    "    model=QWEN_MOE_MODEL_ID,                 \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "        {\"role\": \"user\",   \"content\": \"What are the benefits of renewable energy?\"}\n",
    "    ],\n",
    "    temperature=0,\n",
    "    max_completion_tokens=1500,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "# Extract and print the response text in real-time.\n",
    "print(\"🤖 Streaming Qwen-3-235B-A22B MoE model response:\")\n",
    "for chunk in streaming_response:\n",
    "    if chunk.choices[0].delta.content is not None:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### What's Happening Behind the Scenes?\n",
    "\n",
    "When you use the OpenAI SDK with Bedrock, your requests are automatically translated to Bedrock's native `InvokeModel` API.\n",
    "\n",
    "#### Request Translation\n",
    "- **OpenAI SDK Request** → **Bedrock InvokeModel** \n",
    "- The request body structure remains the same\n",
    "- But there are some key differences in how parameters are handled:\n",
    "\n",
    "| Parameter | OpenAI SDK | Bedrock InvokeModel |\n",
    "|-----------|------------|-------------------|\n",
    "| **Model ID** | In request body | Part of the URL path |\n",
    "| **Streaming** | `stream=True/False` | Different API endpoints:<br/>• `InvokeModel` (non-streaming)<br/>• `InvokeModelWithResponseStream` (streaming) |\n",
    "| **Request Body** | Full chat completions format | Same format, but `model` and `stream` are optional |\n",
    "| **Thinking Mode** | `reasoning_effort='high'/ do not set` | Qwen3-32B Dense model specific parameter |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Enhanced Function Calling with OpenAI SDK\n",
    "\n",
    "Both Qwen models feature enhanced tool calling capabilities for superior performance in agent-based tasks. Let's demonstrate this with a weather lookup function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Weather function and tools configuration ready!\n"
     ]
    }
   ],
   "source": [
    "def get_weather(location):\n",
    "    \"\"\"\n",
    "    Get current weather for a given location.\n",
    "    This is a mock function that returns sample weather data.\n",
    "    \n",
    "    Args:\n",
    "        location (str): City and country, e.g. \"Paris, France\"\n",
    "        \n",
    "    Returns:\n",
    "        dict: Weather information\n",
    "    \"\"\"\n",
    "    # Mock weather data - in a real application, you'd call a weather API\n",
    "    weather_data = {\n",
    "        \"Paris, France\": {\"temperature\": \"22°C\", \"condition\": \"Partly cloudy\", \"humidity\": \"65%\"},\n",
    "        \"New York, USA\": {\"temperature\": \"18°C\", \"condition\": \"Sunny\", \"humidity\": \"45%\"},\n",
    "        \"Tokyo, Japan\": {\"temperature\": \"25°C\", \"condition\": \"Rainy\", \"humidity\": \"80%\"},\n",
    "        \"London, UK\": {\"temperature\": \"15°C\", \"condition\": \"Overcast\", \"humidity\": \"70%\"},\n",
    "        \"Sydney, Australia\": {\"temperature\": \"28°C\", \"condition\": \"Clear\", \"humidity\": \"55%\"}\n",
    "    }\n",
    "    \n",
    "    return weather_data.get(location, {\n",
    "        \"temperature\": \"20°C\", \n",
    "        \"condition\": \"Data not available\", \n",
    "        \"humidity\": \"50%\"\n",
    "    })\n",
    "\n",
    "# Define the function schema for OpenAI SDK\n",
    "tools = [{\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current temperature and weather conditions for a given location.\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City and country, e.g. 'Paris, France'\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}]\n",
    "\n",
    "print(\"✅ Weather function and tools configuration ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Enhanced function calling handler ready!\n"
     ]
    }
   ],
   "source": [
    "def chat_with_functions(client, model, messages, tools, max_iterations=3, reasoning_effort='minimal'):\n",
    "    \"\"\"\n",
    "    Chat with function calling support using OpenAI SDK format.\n",
    "    \n",
    "    Args:\n",
    "        client: OpenAI client instance\n",
    "        model: Model ID to use\n",
    "        messages: List of conversation messages\n",
    "        tools: List of available tools/functions\n",
    "        max_iterations: Maximum number of function call iterations\n",
    "        reasoning_effort: Whether to use thinking mode for enhanced reasoning (MoE model only)\n",
    "        \n",
    "    Returns:\n",
    "        Final assistant message\n",
    "    \"\"\"\n",
    "    \n",
    "    for iteration in range(max_iterations):\n",
    "        print(f\"🔄 Iteration {iteration + 1}\")\n",
    "        \n",
    "        # Make request with tools\n",
    "        request_params = {\n",
    "            \"model\": model,\n",
    "            \"messages\": messages,\n",
    "            \"tools\": tools,\n",
    "            \"tool_choice\": \"auto\"\n",
    "        }\n",
    "        \n",
    "        # Add reasoning_effort only for dense model\n",
    "        if reasoning_effort and \"qwen3-32b-v1\" in model:\n",
    "            request_params[\"reasoning_effort\"] = reasoning_effort\n",
    "        \n",
    "        response = client.chat.completions.create(**request_params)\n",
    "        \n",
    "        assistant_message = response.choices[0].message\n",
    "        messages.append(assistant_message)\n",
    "        \n",
    "        # Check if the model wants to call functions\n",
    "        if assistant_message.tool_calls:\n",
    "            print(f\"🔧 Model requested {len(assistant_message.tool_calls)} function call(s)\")\n",
    "            \n",
    "            # Process each function call\n",
    "            for tool_call in assistant_message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                print(f\"🔧 Calling function: {function_name}\")\n",
    "                print(f\"🔧 Arguments: {function_args}\")\n",
    "                \n",
    "                # Call the actual function\n",
    "                if function_name == \"get_weather\":\n",
    "                    function_result = get_weather(function_args[\"location\"])\n",
    "                    print(f\"🔧 Function result: {function_result}\")\n",
    "                else:\n",
    "                    function_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
    "                \n",
    "                # Add function result to conversation\n",
    "                function_message = {\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"content\": json.dumps(function_result)\n",
    "                }\n",
    "                messages.append(function_message)\n",
    "                \n",
    "        else:\n",
    "            # No more function calls, return final response\n",
    "            print(\"✅ No function calls requested, conversation complete\")\n",
    "            return assistant_message\n",
    "    \n",
    "    print(\"⚠️ Maximum iterations reached\")\n",
    "    return assistant_message\n",
    "\n",
    "print(\"✅ Enhanced function calling handler ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test enhanced function calling with both models\n",
    "weather_questions = [\n",
    "    \"What's the weather like in Paris today?\",\n",
    "    \"Can you tell me the temperature in Tokyo?\",\n",
    "    \"How's the weather in Sydney, Australia?\",\n",
    "    \"What are the conditions like in New York?\"\n",
    "]\n",
    "\n",
    "print(\"🌤️ Testing Enhanced Function Calling with Qwen Models\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Test with Qwen3-32B Dense model\n",
    "print(\"\\n🤖 Testing with Qwen3-32B Dense Model (with thinking mode)\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for i, question in enumerate(weather_questions[:2], 1):  # Test first 2 questions\n",
    "    print(f\"\\n📝 Test {i}: {question}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Create conversation messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. Use the get_weather function to provide accurate weather information.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # Call the function calling handler with dense model\n",
    "        final_response = chat_with_functions(\n",
    "            client=client,\n",
    "            model=QWEN_DENSE_MODEL_ID,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "            reasoning_effort='high'  # Enable thinking mode for dense model\n",
    "        )\n",
    "        \n",
    "        # Print the final response\n",
    "        print(\"🤖 Final response:\")\n",
    "        print(final_response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "    \n",
    "    print()\n",
    "\n",
    "# Test with Qwen-3-235B-A22B MoE model\n",
    "print(\"\\n🧠 Testing with Qwen-3-235B-A22B MoE Model\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i, question in enumerate(weather_questions[2:], 1):  # Test last 2 questions\n",
    "    print(f\"\\n📝 Test {i}: {question}\")\n",
    "    print(\"-\" * 20)\n",
    "    \n",
    "    try:\n",
    "        # Create conversation messages\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful weather assistant. Use the get_weather function to provide accurate weather information.\"},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "        \n",
    "        # Call the function calling handler with MoE model and thinking mode\n",
    "        final_response = chat_with_functions(\n",
    "            client=client,\n",
    "            model=QWEN_MOE_MODEL_ID,\n",
    "            messages=messages,\n",
    "            tools=tools,\n",
    "        )\n",
    "        \n",
    "        # Print the final response\n",
    "        print(\"🧠 Final response:\")\n",
    "        print(final_response.content)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error: {str(e)}\")\n",
    "    \n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### What Just Happened with Enhanced Function Calling?\n",
    "\n",
    "The enhanced function calling demonstration shows both Qwen models' improved capabilities:\n",
    "\n",
    "1. **Enhanced Tool Recognition**: Both models have superior tool calling performance for agent-based tasks\n",
    "2. **Hybrid Thinking Integration**: The dense model can reason through complex tool selection and usage when `reasoning_effort='high'`\n",
    "3. **Improved Function Execution**: Better understanding of when and how to use available tools\n",
    "4. **Multi-step Reasoning**: Both models can plan and execute complex multi-tool workflows\n",
    "5. **Context Awareness**: Enhanced understanding of conversation context for better tool usage decisions\n",
    "\n",
    "**Key Advantages of Qwen Models' Enhanced Tool Calling:**\n",
    "- More accurate tool selection based on user intent\n",
    "- Better handling of complex multi-step agent workflows  \n",
    "- Improved reasoning about tool parameters and results\n",
    "- Enhanced error handling and recovery in tool usage scenarios\n",
    "- **MoE Model (235B)**: Optimized for direct instruction-following in agent tasks\n",
    "- **Dense Model (32B)**: Hybrid thinking mode for complex reasoning and tool planning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Option 2: Amazon Bedrock's InvokeModel API\n",
    "\n",
    "The Bedrock InvokeModel API is the foundational interface for interacting directly with any model hosted on Amazon Bedrock. It provides low-level, flexible access to model inference, allowing you to send input data and receive generated responses in a consistent way across all supported models.\n",
    "\n",
    "**Key Benefits:**\n",
    "- Direct Access: Interact with any Bedrock model using a unified API endpoint.\n",
    "- Fine-Grained Control: Customize inference parameters and payloads for each request.\n",
    "- Streaming Support: Use `InvokeModelWithResponseStream` for real-time, token-by-token output.\n",
    "- Privacy: Amazon Bedrock does not store your input or output data—requests are used only for inference.\n",
    "- **Thinking Mode Control**: Direct control over Qwen-3-235B-A22B MoE model's thinking vs non-thinking behavior.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Setup client\n",
    "\n",
    "First, we setup the Amazon Bedrock client.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📍 Using region: None - change the region variable above to use a different supported region\n"
     ]
    }
   ],
   "source": [
    "# Configure region for Bedrock client\n",
    "region = None\n",
    "\n",
    "if region is None:\n",
    "    target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
    "else:\n",
    "    target_region = \"us-west-2\"\n",
    "\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)\n",
    "print(f\"📍 Using region: {target_region} - change the region variable above to use a different supported region\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Inference with InvokeModel API\n",
    "\n",
    "Then we use the InvokeModel API to perform model inference with the two Qwen models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_model(body, model_id, accept, content_type):\n",
    "    \"\"\"\n",
    "    Invokes Amazon bedrock model to run an inference\n",
    "    using the input provided in the request body.\n",
    "    \n",
    "    Args:\n",
    "        body (dict): The invokation body to send to bedrock\n",
    "        model_id (str): the model to query\n",
    "        accept (str): input accept type\n",
    "        content_type (str): content type\n",
    "    Returns:\n",
    "        Inference response from the model.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            body=json.dumps(body), \n",
    "            modelId=model_id, \n",
    "            accept=accept, \n",
    "            contentType=content_type\n",
    "        )\n",
    "\n",
    "        return response\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Couldn't invoke {model_id}\")\n",
    "        raise e\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example with Qwen-3-235B-A22B MoE model\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "]\n",
    "\n",
    "body = {\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 1000,\n",
    "}\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, QWEN_MOE_MODEL_ID, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(\"🧠 Thinking mode response:\")\n",
    "print(response_body['choices'][0]['message']['content'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Response:\n",
      "<reasoning>\n",
      "Okay, so I need to figure out what the largest city in the southern hemisphere is. Let me start by recalling what the southern hemisphere includes. It's all the countries and regions that are located south of the equator. That includes parts of South America, Africa, Australia, and Antarctica. But since we're talking about cities, Antarctica probably doesn't count because it doesn't have permanent residents.\n",
      "\n",
      "Now, the largest city by population. I know that the largest cities in the world are usually in the northern hemisphere, like Tokyo, Delhi, Shanghai, etc. But I need to check if any of these are in the southern hemisphere. For example, Sydney is in Australia, which is in the southern hemisphere. But is Sydney the largest? I think Australia's largest city is Sydney, but maybe there are other cities in other southern hemisphere countries that are bigger.\n",
      "\n",
      "Wait, Brazil is in the southern hemisphere. São Paulo is a huge city. I remember that São Paulo is one of the largest cities in the world. Let me think. The 2020 estimates for São Paulo's population were around 22 million in the metro area. How does that compare to other southern hemisphere cities?\n",
      "\n",
      "In South Africa, Johannesburg is a major city, but I don't think it's as large as São Paulo. In India, most of the country is in the northern hemisphere, so cities like Mumbai or Delhi are in the north. Australia's Sydney has a population of about 5 million, which is much smaller than São Paulo. \n",
      "\n",
      "What about cities in other southern hemisphere countries? Buenos Aires in Argentina is another big city, but again, probably not as large as São Paulo. Cape Town in South Africa is another one, but again, smaller. \n",
      "\n",
      "So, putting this together, São Paulo, Brazil, is likely the largest city in the southern hemisphere. But I should verify the exact population numbers. Let me check some sources. According to the United Nations, as of 2023, São Paulo's metropolitan area has a population of approximately 22 million. That's the largest in the southern hemisphere. Other cities like Lagos in Nigeria (which is in the northern hemisphere) and Kinshasa in the Democratic Republic of the Congo (also in the northern hemisphere) are large but not in the southern hemisphere. \n",
      "\n",
      "Wait, Nigeria is in the northern hemisphere, so Lagos isn't in the southern. So, yes, São Paulo is the largest. Another point to consider: sometimes the definition of \"city\" can vary—whether it's the city proper, the metro area, or the urban agglomeration. São Paulo's metro area is definitely the largest in the southern hemisphere. \n",
      "\n",
      "I think that's it. The answer should be São Paulo, Brazil.\n",
      "</reasoning> city in the southern hemisphere is **São Paulo, Brazil**. With a metropolitan population of approximately 22 million people (as of recent estimates), it is the most populous urban area in the southern hemisphere. Other major southern hemisphere cities, such as Sydney (Australia) and Johannesburg (South Africa), have significantly smaller populations in comparison.\n"
     ]
    }
   ],
   "source": [
    "# Example with Qwen3-32B Dense model and thinking mode\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "]\n",
    "\n",
    "body = {\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 1000,\n",
    "    \"reasoning_effort\": 'high'  \n",
    "\n",
    "}\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "response = invoke_model(body, QWEN_DENSE_MODEL_ID, accept, contentType)\n",
    "response_body = json.loads(response.get(\"body\").read())\n",
    "\n",
    "print(\"📝 Response:\")\n",
    "print(response_body['choices'][0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Streaming with InvokeModel API\n",
    "\n",
    "The InvokeModel API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Streaming thinking mode response:\n",
      "Time to first token: 0:00:00.910533\n",
      "<reasoning>\n",
      "Okay, so I need to figure out what the largest city in the southern hemisphere is. Let me start by recalling what the southern hemisphere includes. It's all the countries and regions that are located</reasoning><reasoning> south of the equator. That includes parts of South America, Africa, Australia, and Antarctica. But since we're talking about cities, Antarctica probably doesn't count</reasoning><reasoning> because it doesn't have permanent residents.\n",
      "\n",
      "Now, the largest city by population. I know that the largest cities in the world are usually in the northern hemisphere, like Tokyo, Delhi</reasoning><reasoning>, Shanghai, etc. But I need to check if any of these are in the southern hemisphere. Wait, Tokyo is in Japan, which is in the northern hemisphere. Delhi is in India, also northern. Shanghai is in China, northern</reasoning><reasoning>. So maybe the largest city in the southern hemisphere is one of the big cities in South America, Africa, or Australia.\n",
      "\n",
      "Let me think about South America first. The largest cities there are São Paulo and Buenos Aires.</reasoning><reasoning> São Paulo is in Brazil, which is in the southern hemisphere. I remember that São Paulo is one of the biggest cities in the world. Let me check the population. I think São Paulo's population is over 20 million in the metro area. That</reasoning><reasoning>'s pretty big.\n",
      "\n",
      "In Africa, the largest cities are Lagos, Kinshasa, and Cairo. Wait, Cairo is in Egypt, which is in the northern hemisphere. So Lagos and</reasoning><reasoning> Kinshasa are in the southern hemisphere? Wait, no, Africa is mostly in the northern hemisphere, but the southern part is in the southern hemisphere. Lagos is in Nigeria</reasoning><reasoning>, which is in the northern hemisphere. Kinshasa is in the Democratic Republic of the Congo, which is straddling the equator. So maybe part of it is in the southern hemisphere. But I'm not sure if the city itself</reasoning><reasoning> is mostly in the southern hemisphere. Maybe the metro area crosses the equator. But I need to confirm if the city is considered to be in the southern hemisphere.\n",
      "\n",
      "Then there's Johannesburg in South</reasoning><reasoning> Africa, which is definitely in the southern hemisphere. But I think Johannesburg's population is around 5 million, which is smaller than São Paulo.\n",
      "\n",
      "In Australia, the largest cities are Sydney and Melbourne. But Australia is</reasoning><reasoning> entirely in the southern hemisphere. However, the population of Sydney is around 5 million, which is again smaller than São Paulo.\n",
      "\n",
      "So, putting this together, São Paulo in Brazil is a city</reasoning><reasoning> in the southern hemisphere with a very large population. Let me check some sources. According to recent data, São Paulo's metropolitan area has a</reasoning><reasoning> population of over 22 million, making it the largest city in the southern hemisphere. Other cities like Lagos and Kinshasa are also large, but their populations are a bit lower.</reasoning><reasoning> For example, Lagos has around 15 million, and Kinshasa around 16 million. So São Paulo is still the largest.\n",
      "\n",
      "Wait, but I should also consider if there are any other</reasoning><reasoning> cities in the southern hemisphere that I might have missed. For example, in South America, there's also Rio de Janeiro, but that's smaller than São Paulo. In Africa, maybe there's another city. But I think São Paulo is the biggest. Also</reasoning><reasoning>, in Australia, the population is more spread out, so the cities aren't as large as</reasoning><reasoning> in South America or Africa.\n",
      "\n",
      "Therefore, the answer should be São Paulo, Brazil.\n",
      "</reasoning> in the southern hemisphere is **São Paulo, Brazil**. \n",
      "\n",
      "- **Population**: Over 22 million in the metropolitan area (as of recent estimates).  \n",
      "- **Location**: Entirely in the southern hemisphere.  \n",
      "- **Comparison**: Larger than other major southern hemisphere cities like Lagos (Nigeria) and Kinshasa (DRC).  \n",
      "\n",
      "São Paulo is the most populous city in South America and one of the world's largest urban agglomerations.\n",
      "Total chunks: 21\n"
     ]
    }
   ],
   "source": [
    "# Streaming with Qwen3-32B Dense model and thinking mode\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a concise, highly logical assistant.\"},\n",
    "    {\"role\": \"user\",   \"content\": \"What is the largest city in the southern hemisphere?\"}\n",
    "]\n",
    "\n",
    "body = {\n",
    "    \"messages\": messages,\n",
    "    \"temperature\": 0,\n",
    "    \"max_completion_tokens\": 1000,\n",
    "    \"reasoning_effort\": 'high'  # Enable thinking mode for streaming\n",
    "}\n",
    "\n",
    "accept = \"application/json\"\n",
    "contentType = \"application/json\"\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "response = bedrock_runtime.invoke_model_with_response_stream(\n",
    "    body=json.dumps(body), modelId=QWEN_DENSE_MODEL_ID, accept=accept, contentType=contentType\n",
    ")\n",
    "chunk_count = 0\n",
    "time_to_first_token = None\n",
    "\n",
    "# Process the response stream\n",
    "stream = response.get(\"body\")\n",
    "if stream:\n",
    "    print(\"🧠 Streaming thinking mode response:\")\n",
    "    for event in stream:\n",
    "        chunk = event.get(\"chunk\")\n",
    "        if chunk:\n",
    "            # Print the response chunk\n",
    "            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
    "            content_block_delta = chunk_json.get(\"choices\")[0][\"delta\"].get(\"content\")\n",
    "            if content_block_delta:\n",
    "                if time_to_first_token is None:\n",
    "                    time_to_first_token = datetime.now() - start_time\n",
    "                    print(f\"Time to first token: {time_to_first_token}\")\n",
    "\n",
    "                chunk_count += 1\n",
    "                print(content_block_delta, end=\"\")\n",
    "    print(f\"\\nTotal chunks: {chunk_count}\")\n",
    "else:\n",
    "    print(\"No response stream received.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "### Option 3: Amazon Bedrock's Converse API\n",
    "\n",
    "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes. \n",
    "\n",
    "Key Benefits:\n",
    "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
    "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
    "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
    "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management\n",
    "- **Thinking Mode Support**: Direct control over Qwen-3-235B-A22B's thinking capabilities\n",
    "\n",
    "Additionally, the Converse API automatically separates the reasoning trace from the final response, giving developers the flexibility to show or hide the model's thinking process from end users based on their application needs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤖 MOE model response:\n",
      "The capital of Australia is Canberra.\n"
     ]
    }
   ],
   "source": [
    "# Converse API with Qwen-3-235B-A22B MoE model (no thinking mode)\n",
    "response = bedrock_client.converse(\n",
    "    modelId=QWEN_MOE_MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"What is the capital of Australia?\"}]\n",
    "        }\n",
    "    ],\n",
    "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0,\n",
    "        \"maxTokens\": 1000\n",
    "    }\n",
    "    # Note: No additionalModelRequestFields needed for dense model\n",
    ")\n",
    "\n",
    "# Final response (dense model doesn't have reasoning trace)\n",
    "print(f\"🤖 MOE model response:\")\n",
    "print(response['output']['message']['content'][0]['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 Message dict:\n",
      "[{'reasoningContent': {'reasoningText': {'text': \"\\nOkay, the user is asking how far the Moon is from Earth. Let me start by recalling the average distance. I think it's around 384,400 kilometers. But wait, the Moon's orbit isn't a perfect circle, so the distance varies. The closest point is called perigee, and the farthest is apogee. I should mention both to be accurate.\\n\\nWait, what are the exact numbers for perigee and apogee? I remember that perigee is about 363,300 km and apogee is around 405,500 km. Let me double-check those numbers to make sure. Yeah, that's right. So the average is in between those two. \\n\\nAlso, the user might be interested in the unit of measurement. They asked for the distance in kilometers, but sometimes it's also expressed in miles. Maybe I should include both units for clarity. Let me convert 384,400 km to miles. 1 kilometer is approximately 0.621371 miles, so multiplying 384,400 by 0.621371 gives roughly 238,855 miles. \\n\\nI should also consider if the user is looking for a simple answer or more context. Since the question is straightforward, providing the average distance with the range and units should suffice. Maybe add a note about the Moon's elliptical orbit to explain the variation. \\n\\nIs there anything else? The time it takes for light to travel from Earth to the Moon? That's about 1.3 seconds, but maybe that's extra. The user didn't ask for that, so maybe stick to the distance. \\n\\nWait, the user might be a student or someone with a casual interest. Keeping it concise but informative is key. Avoid technical jargon unless necessary. So the answer should state the average distance, mention the range due to the elliptical orbit, and provide both kilometers and miles. That should cover it.\\n\"}}}, {'text': \" is, on average, **384,400 kilometers (238,855 miles)** from Earth. However, this distance varies due to the Moon's elliptical orbit:  \\n- **Closest (perigee):** ~363,300 km (~225,700 miles)  \\n- **Farthest (apogee):** ~405,500 km (~252,000 miles)  \\n\\nLight takes about **1.3 seconds** to travel between Earth and the Moon.\"}]\n",
      "📝 Reasoning trace:\n",
      "\n",
      "Okay, the user is asking how far the Moon is from Earth. Let me start by recalling the average distance. I think it's around 384,400 kilometers. But wait, the Moon's orbit isn't a perfect circle, so the distance varies. The closest point is called perigee, and the farthest is apogee. I should mention both to be accurate.\n",
      "\n",
      "Wait, what are the exact numbers for perigee and apogee? I remember that perigee is about 363,300 km and apogee is around 405,500 km. Let me double-check those numbers to make sure. Yeah, that's right. So the average is in between those two. \n",
      "\n",
      "Also, the user might be interested in the unit of measurement. They asked for the distance in kilometers, but sometimes it's also expressed in miles. Maybe I should include both units for clarity. Let me convert 384,400 km to miles. 1 kilometer is approximately 0.621371 miles, so multiplying 384,400 by 0.621371 gives roughly 238,855 miles. \n",
      "\n",
      "I should also consider if the user is looking for a simple answer or more context. Since the question is straightforward, providing the average distance with the range and units should suffice. Maybe add a note about the Moon's elliptical orbit to explain the variation. \n",
      "\n",
      "Is there anything else? The time it takes for light to travel from Earth to the Moon? That's about 1.3 seconds, but maybe that's extra. The user didn't ask for that, so maybe stick to the distance. \n",
      "\n",
      "Wait, the user might be a student or someone with a casual interest. Keeping it concise but informative is key. Avoid technical jargon unless necessary. So the answer should state the average distance, mention the range due to the elliptical orbit, and provide both kilometers and miles. That should cover it.\n",
      "\n",
      "📝 Final response:\n",
      " is, on average, **384,400 kilometers (238,855 miles)** from Earth. However, this distance varies due to the Moon's elliptical orbit:  \n",
      "- **Closest (perigee):** ~363,300 km (~225,700 miles)  \n",
      "- **Farthest (apogee):** ~405,500 km (~252,000 miles)  \n",
      "\n",
      "Light takes about **1.3 seconds** to travel between Earth and the Moon.\n"
     ]
    }
   ],
   "source": [
    "# Converse API with Qwen3-32B Dense model and thinking mode\n",
    "response = bedrock_client.converse(\n",
    "    modelId=QWEN_DENSE_MODEL_ID,\n",
    "    messages=[\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": [{\"text\": \"How far from earth is the moon?\"}]\n",
    "        }\n",
    "    ],\n",
    "    system=[{\"text\": \"You are a concise, highly logical assistant.\"}],\n",
    "    inferenceConfig={\n",
    "        \"temperature\": 0,\n",
    "        \"maxTokens\": 1000\n",
    "    },\n",
    "    additionalModelRequestFields={\n",
    "        \"reasoning_effort\": \"high\"# Enable thinking mode for MoE model\n",
    "    }\n",
    ")\n",
    "\n",
    "# Message dict\n",
    "print(f\"📝 Message dict:\")\n",
    "print(response['output']['message']['content'])\n",
    "\n",
    "# Reasoning trace (if available)\n",
    "if 'reasoningContent' in response['output']['message']['content'][0]:\n",
    "    print(f\"📝 Reasoning trace:\")\n",
    "    print(response['output']['message']['content'][0]['reasoningContent']['reasoningText']['text'])\n",
    "\n",
    "# Final response\n",
    "print(f\"📝 Final response:\")\n",
    "print(response['output']['message']['content'][1]['text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "#### Streaming with Converse API\n",
    "\n",
    "The Converse API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user. The output below will contain reasoning trace and final response. Based on your application you might want to hide the reasoning trace from the end user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming through Converse API with Qwen3-32B Dense model with thinking mode\n",
    "def bedrock_model_converse_stream_dense(client, system_prompt, user_prompt, max_tokens=1000, temperature=0, reasoning_effort='high'):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=QWEN_DENSE_MODEL_ID,\n",
    "        messages=[  \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },                        \n",
    "        ],\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens\n",
    "        },\n",
    "        additionalModelRequestFields={\n",
    "            \"reasoning_effort\": \"high\"  # Enable thinking mode for Qwen3-32B Dense model\n",
    "        }\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            if chunk['delta'].get('reasoningContent', None):\n",
    "                print(chunk['delta']['reasoningContent']['text'], end=\"\")\n",
    "            if chunk['delta'].get('text', None):\n",
    "                print(chunk['delta']['text'], end=\"\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming through Converse API with Qwen-3-235B-A22B MoE model (instruct/non-thinking)\n",
    "def bedrock_model_converse_stream_moe(client, system_prompt, user_prompt, max_tokens=1000, temperature=0):\n",
    "    response = \"\"\n",
    "    response = client.converse_stream(\n",
    "        modelId=QWEN_MOE_MODEL_ID,\n",
    "        messages=[  \n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\n",
    "                        \"text\": user_prompt\n",
    "                    }\n",
    "                ]\n",
    "            },                        \n",
    "        ],\n",
    "        system=[{\"text\": system_prompt}],\n",
    "        inferenceConfig={\n",
    "            \"temperature\": temperature,\n",
    "            \"maxTokens\": max_tokens\n",
    "        }\n",
    "        # Note: No additionalModelRequestFields needed for instruct model\n",
    "    )\n",
    "    # Extract and print the response text in real-time.\n",
    "    for event in response['stream']:\n",
    "        if 'contentBlockDelta' in event:\n",
    "            chunk = event['contentBlockDelta']\n",
    "            if chunk['delta'].get('text', None):\n",
    "                print(chunk['delta']['text'], end=\"\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "🧠 Streaming with Qwen3-32B Dense model (thinking mode):\n",
      "\n",
      "Okay, so I need to explain how a computer works in simple terms. Let me start by breaking down the main components of a computer. There's the CPU, memory, storage, input devices, output devices, and maybe the motherboard. But how do these parts work together?\n",
      "\n",
      "First, the CPU is the brain of the computer. It processes instructions. But what does that mean exactly? It fetches instructions from memory, decodes them, and executes them. But how does it do that? Maybe I should mention binary, since computers use 0s and 1s. But I need to keep it simple, so maybe not go into too much detail about binary.\n",
      "\n",
      "Memory, like RAM, is where the computer stores data and programs that are currently in use. When you open a program, it's loaded into RAM so the CPU can access it quickly. Storage, like a hard drive or SSD, is for long-term data. So when you shut down, the data in RAM is lost, but storage keeps it.\n",
      "\n",
      "Input devices like a keyboard or mouse let you interact with the computer. Output devices like a monitor or speakers show you the results. The motherboard connects all these parts together, allowing them to communicate.\n",
      "\n",
      "But how do these parts work together in a process? Let's say you want to open a program. You click on it with the mouse (input), the CPU processes that command, fetches the program from storage into RAM, runs it, and displays the result on the screen (output). \n",
      "\n",
      "Wait, maybe I should explain the basic cycle: input, processing, storage, output. That's a common model. Input devices send data to the CPU, which processes it using memory and storage, then outputs the result. \n",
      "\n",
      "Also, the operating system is important. It manages the hardware and software resources. But maybe that's a bit more advanced. The user asked for simple terms, so maybe just mention that the OS helps the computer run smoothly.\n",
      "\n",
      "I should avoid technical jargon as much as possible. Instead of saying \"fetch-decode-execute cycle,\" maybe say the CPU follows instructions step by step. Also, mention that everything is in binary, but explain that 0s and 1s represent on and off states in the computer's circuits.\n",
      "\n",
      "Let me check if I'm missing anything. The power supply is another component, but maybe that's too detailed. The main idea is the flow of data and how components interact. Maybe use an analogy, like a factory where each part has a role. The CPU is the manager, RAM is the workspace, storage is the warehouse, etc.\n",
      "\n",
      "Wait, the user wants a simple explanation, so maybe the analogy is helpful. Let me structure it step by step: start with input, then processing by the CPU using memory, storage for saving data, and output. Keep it concise and avoid going into too much depth on each component. Make sure to connect the parts together to show the overall process.\n",
      " computer works by taking in information, processing it, and then giving you the results. Here's a simple breakdown of how it all fits together:\n",
      "\n",
      "1. **Input**: You interact with the computer using devices like a **keyboard**, **mouse**, or **touchscreen**. These send your commands (like typing or clicking) to the computer.\n",
      "\n",
      "2. **Processing (The Brain)**: The **CPU (Central Processing Unit)** is the \"brain\" that follows step-by-step instructions to do tasks. It uses **RAM (Random Access Memory)** as a temporary workspace to hold data and programs it’s currently using. For example, when you open a game, the game’s instructions are loaded into RAM so the CPU can quickly access them.\n",
      "\n",
      "3. **Storage**: The **hard drive** or **SSD** is like a long-term memory. It saves your files, apps, and the operating system (like Windows or macOS) even when the computer is off. When you start up, the operating system loads from storage into RAM to run the computer.\n",
      "\n",
      "4. **Output**: The results of the processing are shown through **output devices** like a **monitor**, **speakers**, or a **printer**. For example, if you type a document, the text appears on the screen, and you can print it out.\n",
      "\n",
      "5. **Connecting Everything**: The **motherboard** is the main circuit board that connects all the parts (CPU, RAM, storage, etc.) and lets them communicate. It’s like the nervous system of the computer.\n",
      "\n",
      "**How it all works together**:  \n",
      "Imagine you want to write an email. You type on the keyboard (input), the CPU processes your typing using RAM (processing), saves the draft to the hard drive (storage), and displays the email on the screen (output). The operating system helps manage all these steps smoothly.\n",
      "\n",
      "**Bonus Tip**: Everything inside a computer is based on **binary code** (0s and\n",
      "\n",
      "🤖 Streaming with Qwen-3-235B-A22B MoE model (instruct/non-thinking):\n",
      "Renewable energy offers numerous benefits for the environment, economy, and society. Here are some of the key advantages:\n",
      "\n",
      "1. **Environmental Protection**:\n",
      "   - **Reduces Greenhouse Gas Emissions**: Renewable sources like solar, wind, and hydropower produce little to no carbon dioxide or other greenhouse gases, helping to combat climate change.\n",
      "   - **Improves Air Quality**: Unlike fossil fuels, renewables don’t emit harmful pollutants such as sulfur dioxide, nitrogen oxides, or particulate matter, leading to cleaner air and better public health.\n",
      "   - **Conserves Water Resources**: Most renewable energy technologies use significantly less water than fossil fuel power plants, which require large amounts for cooling.\n",
      "\n",
      "2. **Sustainability and Energy Security**:\n",
      "   - **Inexhaustible Supply**: Renewable energy comes from natural sources like sunlight, wind, and water, which are continuously replenished, unlike finite fossil fuels.\n",
      "   - **Reduces Dependence on Imported Fuels**: By using local renewable resources, countries can enhance their energy independence and reduce vulnerability to global fuel price fluctuations and supply disruptions.\n",
      "\n",
      "3. **Economic Benefits**:\n",
      "   - **Job Creation**: The renewable energy sector creates jobs in manufacturing, installation, maintenance, and research and development, often in rural or underserved areas.\n",
      "   - **Stable Energy Prices**: Renewables have low operating costs and are less susceptible to price volatility compared to fossil fuels, leading to more predictable energy costs over time.\n",
      "   - **Stimulates Innovation and Investment**: Growth in renewables drives technological advancements and attracts investment in clean energy infrastructure.\n",
      "\n",
      "4. **Public Health Improvements**:\n",
      "   - Lower pollution levels from renewable energy reduce respiratory and cardiovascular diseases, leading to fewer hospitalizations and lower healthcare costs.\n",
      "\n",
      "5. **Energy Access and Equity**:\n",
      "   - **Decentralized Power Generation**: Solar panels and small wind turbines can provide electricity to remote or off-grid communities, improving energy access in developing regions.\n",
      "   - **Empowers Communities**: Local energy projects can give communities greater control over their energy supply and economic development.\n",
      "\n",
      "6. **Climate Resilience**:\n",
      "   - Renewable energy systems are often more resilient to extreme weather and climate impacts, especially when combined with energy storage and smart grid technologies.\n",
      "\n",
      "Overall, transitioning to renewable energy supports a cleaner, healthier, and more sustainable future for current and future generations."
     ]
    }
   ],
   "source": [
    "# Example usage of streaming functions\n",
    "\n",
    "print(\"\\n\\n🧠 Streaming with Qwen3-32B Dense model (thinking mode):\")\n",
    "bedrock_model_converse_stream_dense(\n",
    "    client=bedrock_client,\n",
    "    system_prompt=\"You are a helpful assistant that thinks through problems step by step.\",\n",
    "    user_prompt=\"Explain how a computer works in simple terms.\",\n",
    "    reasoning_effort='high'\n",
    ")\n",
    "\n",
    "print(\"\\n\\n🤖 Streaming with Qwen-3-235B-A22B MoE model (instruct/non-thinking):\")\n",
    "bedrock_model_converse_stream_moe(\n",
    "    client=bedrock_client,\n",
    "    system_prompt=\"You are a helpful assistant.\",\n",
    "    user_prompt=\"What are the benefits of renewable energy?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "## Conclusion for Qwen Models\n",
    "\n",
    "You've successfully explored **three powerful ways** to interact with Qwen models on Amazon Bedrock, including comprehensive tool use capabilities and thinking mode!\n",
    "\n",
    "### Model Comparison Summary\n",
    "\n",
    "| Feature | Qwen-3-235B-A22B (MoE) | Qwen3-32B (Dense) |\n",
    "|---------|------------------------|-------------------|\n",
    "| **Total Parameters** | 235B | 32B |\n",
    "| **Active Parameters** | 22B per inference | 32B (all active) |\n",
    "| **Thinking Mode** | ❌ Not supported (instruct/non-thinking) | ✅ Supported (hybrid thinking) |\n",
    "| **Use Cases** | Advanced agent tasks, instruction-following | General-purpose, complex reasoning |\n",
    "| **Compute Efficiency** | High (only 22B active) | Consistent (all 32B active) |\n",
    "| **Performance** | Optimized for direct instruction-following | Optimized for reasoning and thinking |\n",
    "\n",
    "### Key Benefits Achieved\n",
    "\n",
    "✅ **Flexibility**: Three different API approaches for different use cases  \n",
    "✅ **Performance**: Streaming support for improved user experience  \n",
    "✅ **Familiarity**: Use existing OpenAI SDK patterns with AWS infrastructure  \n",
    "✅ **Control**: Direct API access when you need fine-grained customization  \n",
    "✅ **Consistency**: Universal interface that works across all Bedrock models  \n",
    "✅ **Privacy**: AWS Bedrock doesn't store your data - only used for inference  \n",
    "✅ **Tool Integration**: Enhanced function calling capabilities across all three approaches\n",
    "✅ **Practical Comparison**: Side-by-side examples using the same function\n",
    "✅ **Thinking Mode**: Step-by-step reasoning capabilities for complex problems (Dense model)\n",
    "✅ **Architecture**: Both Mixture-of-Experts and Dense designs for different use cases\n",
    "✅ **Model Choice**: Instruct/non-thinking (MoE) and hybrid thinking (Dense) architectures\n",
    "✅ **Cross-Region Inference**: Support for multiple AWS regions\n",
    "✅ **Knowledge Bases**: Integration with Amazon Bedrock Knowledge Bases\n",
    "✅ **Bedrock Studio**: Full integration with Bedrock Studio for development\n",
    "\n",
    "### What's Next?\n",
    "\n",
    "You're now equipped with comprehensive knowledge to choose the right API approach and model for your specific use case. Whether you need:\n",
    "- The **simplicity** of the OpenAI SDK\n",
    "- The **control** of InvokeModel \n",
    "- The **consistency** of Converse API\n",
    "- **Enhanced tool use capabilities** for external integrations\n",
    "- **Thinking mode** for complex reasoning tasks (Dense model)\n",
    "- **Direct instruction-following** for agent-based tasks (MoE model)\n",
    "\n",
    "You have all the tools and examples to build powerful AI applications with Qwen models' advanced capabilities on Amazon Bedrock!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
