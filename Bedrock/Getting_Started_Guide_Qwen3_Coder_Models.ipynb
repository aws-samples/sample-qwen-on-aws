{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "# Qwen3 Coder Models Getting Started Guide on Amazon Bedrock\n",
        "\n",
        "This notebook provides a comprehensive introduction to using Qwen3 Coder models on Amazon Bedrock, including how to leverage the familiar OpenAI SDK interface with Amazon Bedrock. We'll cover how to make API requests, explore available parameters and payload structures, and examine use cases for these specialized coding models. \n",
        "\n",
        "## Model Overview\n",
        "\n",
        "### Qwen3-Coder-480B-A35B-Instruct (MoE)\n",
        "\n",
        "**Parameters:** 480 billion total (Mixture-of-Experts hybrid model with 35B active per inference)\n",
        "\n",
        "**Use Cases:** High-performance coding, agentic workflows, software development, tool integration, multi-file code understanding\n",
        "\n",
        "**Key Features:**\n",
        "- **Long Context Support**: Up to 256K tokens for comprehensive codebase understanding\n",
        "- **Multi-file Code Understanding**: Deep reasoning over large codebases\n",
        "- **Agentic Workflows**: Purpose-built for software development and tool integration\n",
        "- **Hybrid Architecture**: Mixture-of-Experts (MoE) design for optimal performance and compute efficiency\n",
        "- **Enhanced Code Generation**: State-of-the-art results for coding tasks\n",
        "- **Compute Efficient**: Only 35B parameters activated per inference despite 480B total parameters\n",
        "\n",
        "### Qwen3-Coder-30B-A3B-Instruct (MoE)\n",
        "\n",
        "**Parameters:** 30 billion total (Mixture-of-Experts hybrid model with 3B active per inference)\n",
        "\n",
        "**Use Cases:** Cost-sensitive coding deployments, code generation, code understanding, software development\n",
        "\n",
        "**Key Features:**\n",
        "- **Long Context Support**: Up to 256K tokens for comprehensive code understanding\n",
        "- **Cost-Effective**: Strong balance of performance and efficiency\n",
        "- **High-Quality Code Generation**: Excellent code generation and understanding capabilities\n",
        "- **Hybrid Architecture**: Mixture-of-Experts (MoE) design for optimal performance\n",
        "- **Efficient Deployment**: Only 3B parameters activated per inference\n",
        "- **Software Development**: Ideal for cost-sensitive deployments requiring quality code generation\n",
        "\n",
        "## Core Capabilities\n",
        "\n",
        "Both Qwen3 Coder models offer the following characteristics:\n",
        "\n",
        "**Input/Output:** Text-in, text-out \n",
        "\n",
        "**Context Window:** 256,000 tokens (specialized for long-context code understanding)\n",
        "\n",
        "**Model Type:** Specialized coding models with enhanced code generation and understanding\n",
        "\n",
        "**Languages:** English and Chinese\n",
        "\n",
        "**Supported Regions:** see [here](https://docs.aws.amazon.com/bedrock/latest/userguide/models-regions.html)\n",
        "\n",
        "**Tool Calling:** ‚úÖ Supported (Enhanced capabilities for software development)\n",
        "\n",
        "**Bedrock Guardrails** ‚úÖ Supported\n",
        "\n",
        "**Converse API** ‚úÖ Supported\n",
        "\n",
        "**OpenAI Chat Completions API** ‚úÖ Supported\n",
        "\n",
        "**Streaming:** ‚úÖ Supported\n",
        "\n",
        "**Model Evaluation:** ‚úÖ Supported\n",
        "\n",
        "**Agents:** ‚úÖ Supported\n",
        "\n",
        "**Prompt Management:** ‚úÖ Supported\n",
        "\n",
        "**Flows:** ‚úÖ Supported\n",
        "\n",
        "**Batch Inference:** ‚úÖ Supported\n",
        "\n",
        "**Knowledge Bases:** ‚úÖ Supported\n",
        "\n",
        "**Bedrock Studio:** ‚úÖ Supported\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## What You'll Learn in this getting started guide\n",
        "\n",
        "- Options to use Amazon Bedrock for Qwen3 Coder models inference, including:    \n",
        "    - Using the OpenAI SDK with Amazon Bedrock\n",
        "    - Using Amazon Bedrock's InvokeModel API\n",
        "    - Using Amazon Bedrock's Converse API\n",
        "- Understanding request parameters and response structures\n",
        "- Leveraging specialized coding capabilities for software development\n",
        "- Implementing enhanced tool calling capabilities for development workflows\n",
        "- Exploring long-context code understanding (up to 256K tokens)\n",
        "- Comparing performance between Qwen3-Coder-480B-A35B-Instruct and Qwen3-Coder-30B-A3B-Instruct models\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Model Access on Amazon Bedrock\n",
        "\n",
        "Ensure you have the correct IAM permission in order to access Qwen3 Coder models on Amazon Bedrock.\n",
        "\n",
        "## IAM Permissions\n",
        "\n",
        "To use Bedrock models, your AWS credentials need the following permissions:\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "```json\n",
        "{\n",
        "  \"Version\": \"2012-10-17\",\n",
        "  \"Statement\": [\n",
        "    {\n",
        "      \"Effect\": \"Allow\",\n",
        "      \"Action\": [\n",
        "        \"bedrock:InvokeModel\",\n",
        "        \"bedrock:InvokeModelWithResponseStream\"\n",
        "      ],\n",
        "      \"Resource\": [\n",
        "        \"arn:aws:bedrock:*::foundation-model/qwen.qwen3-coder-480b-a35b-v1:0\",\n",
        "        \"arn:aws:bedrock:*::foundation-model/qwen.qwen3-coder-30b-a3b-v1:0\"\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "**Note:** The wildcard (`*`) in the region field covers all supported regions"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 1: Environment Configuration\n",
        "\n",
        "First, we need to install the required packages and tell the OpenAI SDK to talk to Bedrock instead of OpenAI's servers.\n",
        "\n",
        "### Required Imports:\n",
        "- `os` ‚Üí For environment variables\n",
        "- `boto3` ‚Üí For native Bedrock API interactions  \n",
        "- `json` ‚Üí For JSON serialization/deserialization\n",
        "- `datetime` ‚Üí For timestamp tracking and performance measurements\n",
        "- `openai` ‚Üí For OpenAI SDK compatibility with Bedrock\n",
        "- `IPython.display` ‚Üí For enhanced output formatting and streaming demonstrations\n",
        "\n",
        "### Environment Variables:\n",
        "We set two environment variables to redirect the OpenAI SDK:\n",
        "- `AWS_BEARER_TOKEN_BEDROCK` ‚Üí Your Bedrock API key  \n",
        "- `OPENAI_BASE_URL` ‚Üí Bedrock's OpenAI-compatible endpoint\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install boto3 openai ipython\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import boto3\n",
        "import json\n",
        "from openai import OpenAI\n",
        "from datetime import datetime\n",
        "from IPython.display import clear_output, display, display_markdown, Markdown\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Model IDs\n",
        "\n",
        "- **qwen.qwen3-coder-480b-a35b-v1:0** (High-performance coding model with 480B total parameters)\n",
        "- **qwen.qwen3-coder-30b-a3b-v1:0** (Cost-effective coding model with 30B total parameters)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Model Configuration - Qwen3 Coder Models\n",
        "QWEN_CODER_480B_MODEL_ID = \"qwen.qwen3-coder-480b-a35b-v1:0\"  # High-performance coding model\n",
        "QWEN_CODER_30B_MODEL_ID = \"qwen.qwen3-coder-30b-a3b-v1:0\"  # Cost-effective coding model\n",
        "\n",
        "print(f\"‚úÖ Using 480B Coder model: {QWEN_CODER_480B_MODEL_ID}\")\n",
        "print(f\"‚úÖ Using 30B Coder model: {QWEN_CODER_30B_MODEL_ID}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Set environment variables to point to Bedrock\n",
        "# Note: Change the region in the URL to match your preferred region\n",
        "os.environ[\"AWS_BEARER_TOKEN_BEDROCK\"] = \"<insert your bedrock API key>\"\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"<insert your bedrock API key>\"\n",
        "os.environ[\"OPENAI_BASE_URL\"] = \"https://bedrock-runtime.us-west-2.amazonaws.com/openai/v1\"\n",
        "\n",
        "print(\"‚úÖ Environment configured for Bedrock!\")\n",
        "print(\"üìç Using us-west-2 region - change the URL above to use a different region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Step 2: Inference with Amazon Bedrock\n",
        "\n",
        "### Option 1: OpenAI SDK\n",
        "\n",
        "#### Import and Initialize OpenAI Client\n",
        "\n",
        "Now we use the **exact same OpenAI SDK** you're familiar with. The client will automatically read the environment variables we just set.\n",
        "\n",
        "**Key Point**: This is the same OpenAI library, but now it's talking to Amazon Bedrock.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize both clients\n",
        "# Note: Change region_name to match your preferred region\n",
        "client = OpenAI()  # For chat completions API\n",
        "bedrock_client = boto3.client('bedrock-runtime', region_name='us-west-2')  \n",
        "\n",
        "print(\"‚úÖ OpenAI client initialized (pointing to Bedrock)\")\n",
        "print(f\"‚úÖ Bedrock client initialized in region: {bedrock_client.meta.region_name}\")\n",
        "print(\"üìç Change region_name above to use a different supported region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Make API Calls \n",
        "\n",
        "The API call structure is identical to OpenAI:\n",
        "- Same `messages` format with `role` and `content`\n",
        "- Same `model` parameter (but uses Bedrock model IDs)  \n",
        "- Same `stream` parameter for real-time responses\n",
        "- **Specialized for Coding**: Both models are optimized for code generation and understanding\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 1: Qwen3-Coder-30B-A3B-Instruct (cost-effective coding model)\n",
        "response = client.chat.completions.create(\n",
        "    model=QWEN_CODER_30B_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant. Write clean, efficient code with comments.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"Please code a quick sort algorithm in Python\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=2000\n",
        ")\n",
        "\n",
        "# Extract and print the response text\n",
        "print(\"ü§ñ Qwen3-Coder-30B-A3B-Instruct response:\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example 2: Qwen3-Coder-480B-A35B-Instruct (high-performance coding model)\n",
        "response = client.chat.completions.create(\n",
        "    model=QWEN_CODER_480B_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert coding assistant. Write optimized, well-documented code with best practices.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"Please code a quick sort algorithm in Python with detailed comments and time complexity analysis\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=3000\n",
        ")\n",
        "\n",
        "# Extract and print the response text\n",
        "print(\"üöÄ Qwen3-Coder-480B-A35B-Instruct response:\")\n",
        "print(response.choices[0].message.content)\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Process Streaming Response\n",
        "\n",
        "Handle the response exactly like you would with OpenAI. Each `item` in the response is a chunk of the model's output. Both Qwen3 Coder models support streaming for real-time code generation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming with Qwen3-Coder-480B-A35B-Instruct (high-performance model)\n",
        "streaming_response = client.chat.completions.create(\n",
        "    model=QWEN_CODER_480B_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are an expert coding assistant. Write clean, efficient code.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"Please code a binary search algorithm in Python\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=2000,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Extract and print the response text in real-time.\n",
        "print(\"üöÄ Streaming Qwen3-Coder-480B-A35B-Instruct response:\")\n",
        "for chunk in streaming_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming with Qwen3-Coder-30B-A3B-Instruct (cost-effective model)\n",
        "streaming_response = client.chat.completions.create(\n",
        "    model=QWEN_CODER_30B_MODEL_ID,                 \n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful coding assistant.\"},\n",
        "        {\"role\": \"user\",   \"content\": \"Please code a simple bubble sort algorithm in Python\"}\n",
        "    ],\n",
        "    temperature=0,\n",
        "    max_completion_tokens=1500,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "# Extract and print the response text in real-time.\n",
        "print(\"ü§ñ Streaming Qwen3-Coder-30B-A3B-Instruct response:\")\n",
        "for chunk in streaming_response:\n",
        "    if chunk.choices[0].delta.content is not None:\n",
        "        print(chunk.choices[0].delta.content, end=\"\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Enhanced Function Calling for Development Workflows\n",
        "\n",
        "Both Qwen3 Coder models feature enhanced tool calling capabilities specifically designed for software development and agentic workflows. Let's demonstrate this with a code execution function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def execute_python_code(code):\n",
        "    \"\"\"\n",
        "    Execute Python code and return the result.\n",
        "    This is a mock function that simulates code execution.\n",
        "    \n",
        "    Args:\n",
        "        code (str): Python code to execute\n",
        "        \n",
        "    Returns:\n",
        "        dict: Execution result with output and any errors\n",
        "    \"\"\"\n",
        "    # Mock execution - in a real application, you'd use a secure code execution environment\n",
        "    try:\n",
        "        # Simulate successful execution\n",
        "        if \"print\" in code:\n",
        "            return {\n",
        "                \"status\": \"success\",\n",
        "                \"output\": \"Code executed successfully\",\n",
        "                \"result\": \"Hello, World!\" if \"Hello\" in code else \"Output generated\"\n",
        "            }\n",
        "        else:\n",
        "            return {\n",
        "                \"status\": \"success\", \n",
        "                \"output\": \"Code executed without output\",\n",
        "                \"result\": \"No output to display\"\n",
        "            }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            \"status\": \"error\",\n",
        "            \"output\": f\"Execution failed: {str(e)}\",\n",
        "            \"result\": None\n",
        "        }\n",
        "\n",
        "# Define the function schema for OpenAI SDK\n",
        "tools = [{\n",
        "    \"type\": \"function\",\n",
        "    \"function\": {\n",
        "        \"name\": \"execute_python_code\",\n",
        "        \"description\": \"Execute Python code and return the result. Use this to test and validate generated code.\",\n",
        "        \"parameters\": {\n",
        "            \"type\": \"object\",\n",
        "            \"properties\": {\n",
        "                \"code\": {\n",
        "                    \"type\": \"string\",\n",
        "                    \"description\": \"Python code to execute\"\n",
        "                }\n",
        "            },\n",
        "            \"required\": [\"code\"],\n",
        "            \"additionalProperties\": False\n",
        "        }\n",
        "    }\n",
        "}]\n",
        "\n",
        "print(\"‚úÖ Code execution function and tools configuration ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def chat_with_coding_functions(client, model, messages, tools, max_iterations=3):\n",
        "    \"\"\"\n",
        "    Chat with function calling support for coding workflows using OpenAI SDK format.\n",
        "    \n",
        "    Args:\n",
        "        client: OpenAI client instance\n",
        "        model: Model ID to use\n",
        "        messages: List of conversation messages\n",
        "        tools: List of available tools/functions\n",
        "        max_iterations: Maximum number of function call iterations\n",
        "        \n",
        "    Returns:\n",
        "        Final assistant message\n",
        "    \"\"\"\n",
        "    \n",
        "    for iteration in range(max_iterations):\n",
        "        print(f\"üîÑ Iteration {iteration + 1}\")\n",
        "        \n",
        "        # Make request with tools\n",
        "        request_params = {\n",
        "            \"model\": model,\n",
        "            \"messages\": messages,\n",
        "            \"tools\": tools,\n",
        "            \"tool_choice\": \"auto\"\n",
        "        }\n",
        "        \n",
        "        response = client.chat.completions.create(**request_params)\n",
        "        \n",
        "        assistant_message = response.choices[0].message\n",
        "        messages.append(assistant_message)\n",
        "        \n",
        "        # Check if the model wants to call functions\n",
        "        if assistant_message.tool_calls:\n",
        "            print(f\"üîß Model requested {len(assistant_message.tool_calls)} function call(s)\")\n",
        "            \n",
        "            # Process each function call\n",
        "            for tool_call in assistant_message.tool_calls:\n",
        "                function_name = tool_call.function.name\n",
        "                function_args = json.loads(tool_call.function.arguments)\n",
        "                \n",
        "                print(f\"üîß Calling function: {function_name}\")\n",
        "                print(f\"üîß Arguments: {function_args}\")\n",
        "                \n",
        "                # Call the actual function\n",
        "                if function_name == \"execute_python_code\":\n",
        "                    function_result = execute_python_code(function_args[\"code\"])\n",
        "                    print(f\"üîß Function result: {function_result}\")\n",
        "                else:\n",
        "                    function_result = {\"error\": f\"Unknown function: {function_name}\"}\n",
        "                \n",
        "                # Add function result to conversation\n",
        "                function_message = {\n",
        "                    \"tool_call_id\": tool_call.id,\n",
        "                    \"role\": \"tool\",\n",
        "                    \"content\": json.dumps(function_result)\n",
        "                }\n",
        "                messages.append(function_message)\n",
        "                \n",
        "        else:\n",
        "            # No more function calls, return final response\n",
        "            print(\"‚úÖ No function calls requested, conversation complete\")\n",
        "            return assistant_message\n",
        "    \n",
        "    print(\"‚ö†Ô∏è Maximum iterations reached\")\n",
        "    return assistant_message\n",
        "\n",
        "print(\"‚úÖ Enhanced coding function calling handler ready!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test enhanced function calling with both Coder models\n",
        "coding_questions = [\n",
        "    \"Write a Python function to calculate the factorial of a number and test it\",\n",
        "    \"Create a simple calculator function and demonstrate it works\",\n",
        "    \"Write a function to reverse a string and test it with 'Hello World'\",\n",
        "    \"Create a function to find the maximum number in a list and test it\"\n",
        "]\n",
        "\n",
        "print(\"üíª Testing Enhanced Function Calling with Qwen3 Coder Models\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Test with Qwen3-Coder-30B-A3B-Instruct (cost-effective model)\n",
        "print(\"\\nü§ñ Testing with Qwen3-Coder-30B-A3B-Instruct\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, question in enumerate(coding_questions[:2], 1):  # Test first 2 questions\n",
        "    print(f\"\\nüìù Test {i}: {question}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    try:\n",
        "        # Create conversation messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful coding assistant. Write clean code and use the execute_python_code function to test your code.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "        \n",
        "        # Call the function calling handler with 30B model\n",
        "        final_response = chat_with_coding_functions(\n",
        "            client=client,\n",
        "            model=QWEN_CODER_30B_MODEL_ID,\n",
        "            messages=messages,\n",
        "            tools=tools\n",
        "        )\n",
        "        \n",
        "        # Print the final response\n",
        "        print(\"ü§ñ Final response:\")\n",
        "        print(final_response.content)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "    \n",
        "    print()\n",
        "\n",
        "# Test with Qwen3-Coder-480B-A35B-Instruct (high-performance model)\n",
        "print(\"\\nüöÄ Testing with Qwen3-Coder-480B-A35B-Instruct\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i, question in enumerate(coding_questions[2:], 1):  # Test last 2 questions\n",
        "    print(f\"\\nüìù Test {i}: {question}\")\n",
        "    print(\"-\" * 30)\n",
        "    \n",
        "    try:\n",
        "        # Create conversation messages\n",
        "        messages = [\n",
        "            {\"role\": \"system\", \"content\": \"You are an expert coding assistant. Write optimized, well-documented code and use the execute_python_code function to test your code.\"},\n",
        "            {\"role\": \"user\", \"content\": question}\n",
        "        ]\n",
        "        \n",
        "        # Call the function calling handler with 480B model\n",
        "        final_response = chat_with_coding_functions(\n",
        "            client=client,\n",
        "            model=QWEN_CODER_480B_MODEL_ID,\n",
        "            messages=messages,\n",
        "            tools=tools\n",
        "        )\n",
        "        \n",
        "        # Print the final response\n",
        "        print(\"üöÄ Final response:\")\n",
        "        print(final_response.content)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {str(e)}\")\n",
        "    \n",
        "    print()\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### What Just Happened with Enhanced Function Calling?\n",
        "\n",
        "The enhanced function calling demonstration shows both Qwen3 Coder models' specialized capabilities for software development:\n",
        "\n",
        "1. **Enhanced Code Generation**: Both models excel at generating clean, efficient code\n",
        "2. **Tool Integration**: Superior performance in development workflows with external tools\n",
        "3. **Code Testing**: Ability to generate and test code using function calls\n",
        "4. **Multi-step Development**: Both models can plan and execute complex coding workflows\n",
        "5. **Context Awareness**: Enhanced understanding of coding requirements and best practices\n",
        "\n",
        "**Key Advantages of Qwen3 Coder Models' Enhanced Tool Calling:**\n",
        "- More accurate code generation based on requirements\n",
        "- Better handling of complex multi-step development workflows  \n",
        "- Improved reasoning about code structure and implementation\n",
        "- Enhanced error handling and code validation\n",
        "- **480B Model**: State-of-the-art performance for complex coding tasks\n",
        "- **30B Model**: Cost-effective performance for standard coding needs\n",
        "- **Long Context**: Up to 256K tokens for understanding large codebases\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Option 2: Amazon Bedrock's InvokeModel API\n",
        "\n",
        "The Bedrock InvokeModel API is the foundational interface for interacting directly with any model hosted on Amazon Bedrock. It provides low-level, flexible access to model inference, allowing you to send input data and receive generated responses in a consistent way across all supported models.\n",
        "\n",
        "**Key Benefits:**\n",
        "- Direct Access: Interact with any Bedrock model using a unified API endpoint.\n",
        "- Fine-Grained Control: Customize inference parameters and payloads for each request.\n",
        "- Streaming Support: Use `InvokeModelWithResponseStream` for real-time, token-by-token output.\n",
        "- Privacy: Amazon Bedrock does not store your input or output data‚Äîrequests are used only for inference.\n",
        "- **Long Context Support**: Direct control over the 256K token context window for large codebases.\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Setup client\n",
        "\n",
        "First, we setup the Amazon Bedrock client.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Configure region for Bedrock client\n",
        "region = None\n",
        "\n",
        "if region is None:\n",
        "    target_region = os.environ.get(\"AWS_REGION\", os.environ.get(\"AWS_DEFAULT_REGION\"))\n",
        "else:\n",
        "    target_region = \"us-west-2\"\n",
        "\n",
        "bedrock_runtime = boto3.client('bedrock-runtime', region_name=region)\n",
        "print(f\"üìç Using region: {target_region} - change the region variable above to use a different supported region\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Inference with InvokeModel API\n",
        "\n",
        "Then we use the InvokeModel API to perform model inference with the two Qwen3 Coder models.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def invoke_model(body, model_id, accept, content_type):\n",
        "    \"\"\"\n",
        "    Invokes Amazon bedrock model to run an inference\n",
        "    using the input provided in the request body.\n",
        "    \n",
        "    Args:\n",
        "        body (dict): The invokation body to send to bedrock\n",
        "        model_id (str): the model to query\n",
        "        accept (str): input accept type\n",
        "        content_type (str): content type\n",
        "    Returns:\n",
        "        Inference response from the model.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        response = bedrock_runtime.invoke_model(\n",
        "            body=json.dumps(body), \n",
        "            modelId=model_id, \n",
        "            accept=accept, \n",
        "            contentType=content_type\n",
        "        )\n",
        "\n",
        "        return response\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Couldn't invoke {model_id}\")\n",
        "        raise e\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with Qwen3-Coder-480B-A35B-Instruct (high-performance coding model)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert coding assistant. Write clean, efficient code with comments.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"Please code a merge sort algorithm in Python\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 2000\n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "response = invoke_model(body, QWEN_CODER_480B_MODEL_ID, accept, contentType)\n",
        "response_body = json.loads(response.get(\"body\").read())\n",
        "\n",
        "print(\"üöÄ Qwen3-Coder-480B-A35B-Instruct response:\")\n",
        "print(response_body['choices'][0]['message']['content'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example with Qwen3-Coder-30B-A3B-Instruct (cost-effective coding model)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are a helpful coding assistant. Write clean, efficient code.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"Please code a simple insertion sort algorithm in Python\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 1500\n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "response = invoke_model(body, QWEN_CODER_30B_MODEL_ID, accept, contentType)\n",
        "response_body = json.loads(response.get(\"body\").read())\n",
        "\n",
        "print(\"ü§ñ Qwen3-Coder-30B-A3B-Instruct response:\")\n",
        "print(response_body['choices'][0]['message']['content'])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Streaming with InvokeModel API\n",
        "\n",
        "The InvokeModel API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming with Qwen3-Coder-480B-A35B-Instruct (high-performance model)\n",
        "messages = [\n",
        "    {\"role\": \"system\", \"content\": \"You are an expert coding assistant. Write clean, efficient code.\"},\n",
        "    {\"role\": \"user\",   \"content\": \"Please code a heap sort algorithm in Python\"}\n",
        "]\n",
        "\n",
        "body = {\n",
        "    \"messages\": messages,\n",
        "    \"temperature\": 0,\n",
        "    \"max_completion_tokens\": 2000\n",
        "}\n",
        "\n",
        "accept = \"application/json\"\n",
        "contentType = \"application/json\"\n",
        "\n",
        "start_time = datetime.now()\n",
        "\n",
        "response = bedrock_runtime.invoke_model_with_response_stream(\n",
        "    body=json.dumps(body), modelId=QWEN_CODER_480B_MODEL_ID, accept=accept, contentType=contentType\n",
        ")\n",
        "chunk_count = 0\n",
        "time_to_first_token = None\n",
        "\n",
        "# Process the response stream\n",
        "stream = response.get(\"body\")\n",
        "if stream:\n",
        "    print(\"üöÄ Streaming Qwen3-Coder-480B-A35B-Instruct response:\")\n",
        "    for event in stream:\n",
        "        chunk = event.get(\"chunk\")\n",
        "        if chunk:\n",
        "            # Print the response chunk\n",
        "            chunk_json = json.loads(chunk.get(\"bytes\").decode())\n",
        "            content_block_delta = chunk_json.get(\"choices\")[0][\"delta\"].get(\"content\")\n",
        "            if content_block_delta:\n",
        "                if time_to_first_token is None:\n",
        "                    time_to_first_token = datetime.now() - start_time\n",
        "                    print(f\"Time to first token: {time_to_first_token}\")\n",
        "\n",
        "                chunk_count += 1\n",
        "                print(content_block_delta, end=\"\")\n",
        "    print(f\"\\nTotal chunks: {chunk_count}\")\n",
        "else:\n",
        "    print(\"No response stream received.\")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "### Option 3: Amazon Bedrock's Converse API\n",
        "\n",
        "The Bedrock Converse API provides a consistent interface for working with all Bedrock models that support messages. This means you can write your code once and use it across different models without changes. \n",
        "\n",
        "Key Benefits:\n",
        "- Universal Interface: Same API structure works with Claude, Llama, Titan, and other models\n",
        "- Model-Specific Parameters: Pass unique parameters when needed for specific models\n",
        "- Privacy: Amazon Bedrock doesn't store any content you provide - data is only used for response generation\n",
        "- Advanced Features: Built-in support for guardrails, tools/function calling, and prompt management\n",
        "- **Long Context Support**: Direct control over the 256K token context window for large codebases\n",
        "\n",
        "Additionally, the Converse API automatically handles the specialized coding capabilities of Qwen3 Coder models, giving developers the flexibility to leverage the models' enhanced code generation and understanding capabilities.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converse API with Qwen3-Coder-30B-A3B-Instruct (cost-effective model)\n",
        "response = bedrock_client.converse(\n",
        "    modelId=QWEN_CODER_30B_MODEL_ID,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"text\": \"Please code a simple linear search algorithm in Python\"}]\n",
        "        }\n",
        "    ],\n",
        "    system=[{\"text\": \"You are a helpful coding assistant. Write clean, efficient code.\"}],\n",
        "    inferenceConfig={\n",
        "        \"temperature\": 0,\n",
        "        \"maxTokens\": 1500\n",
        "    }\n",
        ")\n",
        "\n",
        "# Final response\n",
        "print(f\"ü§ñ Qwen3-Coder-30B-A3B-Instruct response:\")\n",
        "print(response['output']['message']['content'][0]['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Converse API with Qwen3-Coder-480B-A35B-Instruct (high-performance model)\n",
        "response = bedrock_client.converse(\n",
        "    modelId=QWEN_CODER_480B_MODEL_ID,\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [{\"text\": \"Please code an optimized binary search algorithm in Python with detailed comments\"}]\n",
        "        }\n",
        "    ],\n",
        "    system=[{\"text\": \"You are an expert coding assistant. Write optimized, well-documented code with best practices.\"}],\n",
        "    inferenceConfig={\n",
        "        \"temperature\": 0,\n",
        "        \"maxTokens\": 2000\n",
        "    }\n",
        ")\n",
        "\n",
        "# Final response\n",
        "print(f\"üöÄ Qwen3-Coder-480B-A35B-Instruct response:\")\n",
        "print(response['output']['message']['content'][0]['text'])\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "#### Streaming with Converse API\n",
        "\n",
        "The Converse API comes with built in streaming support. This can be useful in user-facing applications since it reduces time to first token (TTFT) metric and with that perceived inference latency for the end user.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming through Converse API with Qwen3-Coder-30B-A3B-Instruct (cost-effective model)\n",
        "def bedrock_model_converse_stream_30b(client, system_prompt, user_prompt, max_tokens=1500, temperature=0):\n",
        "    response = client.converse_stream(\n",
        "        modelId=QWEN_CODER_30B_MODEL_ID,\n",
        "        messages=[  \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"text\": user_prompt\n",
        "                    }\n",
        "                ]\n",
        "            },                        \n",
        "        ],\n",
        "        system=[{\"text\": system_prompt}],\n",
        "        inferenceConfig={\n",
        "            \"temperature\": temperature,\n",
        "            \"maxTokens\": max_tokens\n",
        "        }\n",
        "    )\n",
        "    # Extract and print the response text in real-time.\n",
        "    for event in response['stream']:\n",
        "        if 'contentBlockDelta' in event:\n",
        "            chunk = event['contentBlockDelta']\n",
        "            if chunk['delta'].get('text', None):\n",
        "                print(chunk['delta']['text'], end=\"\")\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Streaming through Converse API with Qwen3-Coder-480B-A35B-Instruct (high-performance model)\n",
        "def bedrock_model_converse_stream_480b(client, system_prompt, user_prompt, max_tokens=2000, temperature=0):\n",
        "    response = client.converse_stream(\n",
        "        modelId=QWEN_CODER_480B_MODEL_ID,\n",
        "        messages=[  \n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    {\n",
        "                        \"text\": user_prompt\n",
        "                    }\n",
        "                ]\n",
        "            },                        \n",
        "        ],\n",
        "        system=[{\"text\": system_prompt}],\n",
        "        inferenceConfig={\n",
        "            \"temperature\": temperature,\n",
        "            \"maxTokens\": max_tokens\n",
        "        }\n",
        "    )\n",
        "    # Extract and print the response text in real-time.\n",
        "    for event in response['stream']:\n",
        "        if 'contentBlockDelta' in event:\n",
        "            chunk = event['contentBlockDelta']\n",
        "            if chunk['delta'].get('text', None):\n",
        "                print(chunk['delta']['text'], end=\"\")\n",
        "    return\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example usage of streaming functions\n",
        "\n",
        "print(\"\\n\\nü§ñ Streaming with Qwen3-Coder-30B-A3B-Instruct:\")\n",
        "bedrock_model_converse_stream_30b(\n",
        "    client=bedrock_client,\n",
        "    system_prompt=\"You are a helpful coding assistant.\",\n",
        "    user_prompt=\"Please code a simple selection sort algorithm in Python\"\n",
        ")\n",
        "\n",
        "print(\"\\n\\nüöÄ Streaming with Qwen3-Coder-480B-A35B-Instruct:\")\n",
        "bedrock_model_converse_stream_480b(\n",
        "    client=bedrock_client,\n",
        "    system_prompt=\"You are an expert coding assistant. Write optimized, well-documented code.\",\n",
        "    user_prompt=\"Please code an efficient radix sort algorithm in Python with detailed comments\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "## Conclusion for Qwen3 Coder Models\n",
        "\n",
        "You've successfully explored **three powerful ways** to interact with Qwen3 Coder models on Amazon Bedrock, including comprehensive tool use capabilities and specialized coding features!\n",
        "\n",
        "### Model Comparison Summary\n",
        "\n",
        "| Feature | Qwen3-Coder-480B-A35B-Instruct | Qwen3-Coder-30B-A3B-Instruct |\n",
        "|---------|--------------------------------|-------------------------------|\n",
        "| **Total Parameters** | 480B | 30B |\n",
        "| **Active Parameters** | 35B per inference | 3B per inference |\n",
        "| **Use Cases** | High-performance coding, complex software development | Cost-effective coding, standard development tasks |\n",
        "| **Context Window** | 256K tokens | 256K tokens |\n",
        "| **Performance** | State-of-the-art for coding tasks | Strong balance of performance and efficiency |\n",
        "| **Cost Efficiency** | Premium performance | Cost-optimized |\n",
        "\n",
        "### Key Benefits Achieved\n",
        "\n",
        "‚úÖ **Flexibility**: Three different API approaches for different use cases  \n",
        "‚úÖ **Performance**: Streaming support for improved user experience  \n",
        "‚úÖ **Familiarity**: Use existing OpenAI SDK patterns with AWS infrastructure  \n",
        "‚úÖ **Control**: Direct API access when you need fine-grained customization  \n",
        "‚úÖ **Consistency**: Universal interface that works across all Bedrock models  \n",
        "‚úÖ **Privacy**: AWS Bedrock doesn't store your data - only used for inference  \n",
        "‚úÖ **Tool Integration**: Enhanced function calling capabilities for development workflows\n",
        "‚úÖ **Long Context**: Up to 256K tokens for understanding large codebases\n",
        "‚úÖ **Specialized Coding**: Purpose-built for software development and agentic workflows\n",
        "‚úÖ **Model Choice**: Both high-performance and cost-effective options\n",
        "‚úÖ **Cross-Region Inference**: Support for multiple AWS regions\n",
        "‚úÖ **Knowledge Bases**: Integration with Amazon Bedrock Knowledge Bases\n",
        "‚úÖ **Bedrock Studio**: Full integration with Bedrock Studio for development\n",
        "\n",
        "### What's Next?\n",
        "\n",
        "You're now equipped with comprehensive knowledge to choose the right API approach and model for your specific coding use case. Whether you need:\n",
        "- The **simplicity** of the OpenAI SDK\n",
        "- The **control** of InvokeModel \n",
        "- The **consistency** of Converse API\n",
        "- **Enhanced tool use capabilities** for development workflows\n",
        "- **Long-context understanding** for large codebases (up to 256K tokens)\n",
        "- **High-performance coding** with the 480B model\n",
        "- **Cost-effective coding** with the 30B model\n",
        "\n",
        "You have all the tools and examples to build powerful AI-powered development applications with Qwen3 Coder models' specialized capabilities on Amazon Bedrock!\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
