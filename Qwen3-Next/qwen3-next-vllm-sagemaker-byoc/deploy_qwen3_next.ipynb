{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy Qwen3-Next-80B-A3B-Instruct on Amazon SageMaker\n",
    "\n",
    "This notebook demonstrates how to deploy the Qwen3-Next-80B-A3B-Instruct model on Amazon SageMaker using a custom vLLM container.\n",
    "\n",
    "## Model Overview\n",
    "- **Model**: Qwen3-Next-80B-A3B-Instruct\n",
    "- **Parameters**: 80B total (3B activated)\n",
    "- **Context Length**: 262K tokens (extensible to 1M+)\n",
    "- **Architecture**: Hybrid Attention + High-Sparsity MoE (512 experts, 10 activated)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "1. AWS CLI configured with appropriate permissions\n",
    "2. SageMaker execution role with necessary permissions\n",
    "3. Access to ml.g6e.12xlarge for SageMaker endpoint\n",
    "4. Custom Docker image pushed to ECR using ```build_and_push.sh``` script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:52:24.013903Z",
     "iopub.status.busy": "2025-09-18T14:52:24.013537Z",
     "iopub.status.idle": "2025-09-18T14:52:24.016493Z",
     "shell.execute_reply": "2025-09-18T14:52:24.016010Z",
     "shell.execute_reply.started": "2025-09-18T14:52:24.013883Z"
    }
   },
   "outputs": [],
   "source": [
    "# !pip install -r code/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T14:52:24.017284Z",
     "iopub.status.busy": "2025-09-18T14:52:24.017133Z",
     "iopub.status.idle": "2025-09-18T14:52:24.020717Z",
     "shell.execute_reply": "2025-09-18T14:52:24.020337Z",
     "shell.execute_reply.started": "2025-09-18T14:52:24.017270Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U strands-agents strands-agents-tools\n",
    "!pip install pydantic==2.11.7  # Compatible version\n",
    "!pip install mypy-boto3-sagemaker-runtime  # Type stubs for SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import base64\n",
    "import requests\n",
    "import subprocess\n",
    "import os\n",
    "from typing import Dict, List, Any, Optional\n",
    "\n",
    "# Initialize SageMaker session and get execution role\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()  # IAM role for SageMaker operations\n",
    "bucket = \"your-s3-bucket\"  # S3 bucket for storing model artifacts\n",
    "\n",
    "# Get AWS account and region information\n",
    "sts_client = boto3.client('sts')\n",
    "account_id = sts_client.get_caller_identity()['Account']\n",
    "region = boto3.Session().region_name\n",
    "\n",
    "print(f\"AWS Account ID: {account_id}\")\n",
    "print(f\"AWS Region: {region}\")\n",
    "print(f\"SageMaker role: {role}\")\n",
    "print(f\"S3 bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T16:28:45.178265Z",
     "iopub.status.busy": "2025-09-18T16:28:45.177917Z",
     "iopub.status.idle": "2025-09-18T16:28:45.181418Z",
     "shell.execute_reply": "2025-09-18T16:28:45.180956Z",
     "shell.execute_reply.started": "2025-09-18T16:28:45.178248Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Custom container will be built and pushed to:\n",
      "Repository: qwen-vllm-byoc\n",
      "Image URI: 459006231907.dkr.ecr.us-west-2.amazonaws.com/qwen-vllm-byoc:latest\n"
     ]
    }
   ],
   "source": [
    "# Configuration for custom container\n",
    "repository_name = \"qwen-vllm-byoc\"\n",
    "image_tag = \"latest\"\n",
    "image_uri = f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{repository_name}:{image_tag}\"\n",
    "\n",
    "print(f\"Custom container will be built and pushed to:\")\n",
    "print(f\"Repository: {repository_name}\")\n",
    "print(f\"Image URI: {image_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üéØ Model and Endpoint Configuration\n",
    "model_name = \"qwen3-next-80b-a3b-instruct\"\n",
    "endpoint_name = f\"{model_name}-{int(time.time())}\"  # Unique endpoint name with timestamp\n",
    "\n",
    "# üñ•Ô∏è Instance Configuration\n",
    "instance_type = \"ml.g6e.12xlarge\"  # 4x NVIDIA L40S GPUs, 192GB RAM\n",
    "instance_count = 1                  # Single instance deployment\n",
    "\n",
    "# üóÑÔ∏è Storage Configuration  \n",
    "prefix = \"qwen3-next-deployment\"              # S3 prefix for organization\n",
    "byoc_code_dir = \"./code\"                      # Local directory with model files\n",
    "\n",
    "print(f\"üöÄ Deployment Configuration:\")\n",
    "print(f\"   Model Name: {model_name}\")\n",
    "print(f\"   Endpoint Name: {endpoint_name}\")\n",
    "print(f\"   Instance Type: {instance_type}\")\n",
    "print(f\"   Instance Count: {instance_count}\")\n",
    "print(f\"   S3 Bucket: {bucket}\")\n",
    "print(f\"   S3 Prefix: {prefix}\")\n",
    "print(f\"   Code Directory: {byoc_code_dir}\")\n",
    "\n",
    "# Verify code directory exists\n",
    "import os\n",
    "if os.path.exists(byoc_code_dir):\n",
    "    files = os.listdir(byoc_code_dir)\n",
    "    print(f\"   Code Files: {files}\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Warning: Code directory '{byoc_code_dir}' not found!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Upload Model Configuration Files to S3\n",
    "\n",
    "For BYOC deployments, we need to upload our custom model configuration files to S3:\n",
    "\n",
    "- **`model.py`**: Inference server with vLLM integration\n",
    "- **`serving.properties`**: vLLM engine configuration parameters\n",
    "- **`requirements.txt`**: Python dependencies for the container\n",
    "\n",
    "These files will be downloaded by the container during startup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload BYOC configuration files to S3\n",
    "print(\"üì§ Uploading model configuration files to S3...\")\n",
    "\n",
    "byoc_config_uri = sagemaker_session.upload_data(\n",
    "    path=byoc_code_dir, \n",
    "    bucket=bucket, \n",
    "    key_prefix=f\"{prefix}/code\"\n",
    ")\n",
    "\n",
    "# Prepare model data configuration for SageMaker\n",
    "model_data = {\n",
    "    \"S3DataSource\": {\n",
    "        \"S3Uri\": f\"{byoc_config_uri}/\",\n",
    "        \"S3DataType\": \"S3Prefix\",         # Directory containing multiple files\n",
    "        \"CompressionType\": \"None\"         # Files are not compressed\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"‚úÖ Upload completed!\")\n",
    "print(f\"   S3 URI: {byoc_config_uri}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create SageMaker Model Configuration\n",
    "\n",
    "Now we'll create the SageMaker model configuration that combines our custom container with the uploaded configuration files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T16:29:09.472317Z",
     "iopub.status.busy": "2025-09-18T16:29:09.471801Z",
     "iopub.status.idle": "2025-09-18T16:29:09.476270Z",
     "shell.execute_reply": "2025-09-18T16:29:09.475859Z",
     "shell.execute_reply.started": "2025-09-18T16:29:09.472300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üèóÔ∏è Creating SageMaker model configuration...\n",
      "‚úÖ SageMaker model created successfully!\n"
     ]
    }
   ],
   "source": [
    "# Create SageMaker model configuration\n",
    "print(\"üèóÔ∏è Creating SageMaker model configuration...\")\n",
    "\n",
    "model = Model(\n",
    "    image_uri=image_uri,                        # Custom vLLM container\n",
    "    model_data=model_data,                           # S3 path to configuration files\n",
    "    role=role,                                       # SageMaker execution role\n",
    "    name=f\"{model_name}-model-{int(time.time())}\",   # Unique model name\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    env={\n",
    "        # üöÄ vLLM Configuration\n",
    "        'VLLM_USE_V1': '1',                          # Use vLLM v1 engine (required for Qwen3-Next)\n",
    "        'VLLM_WORKER_MULTIPROC_METHOD': 'spawn',     # Process spawning method\n",
    "        'VLLM_DISTRIBUTED_EXECUTOR_BACKEND': 'mp',   # Multi-processing backend\n",
    "        'VLLM_LOGGING_LEVEL': 'INFO',                # Logging level\n",
    "        \n",
    "        # üñ•Ô∏è GPU Configuration  \n",
    "        'CUDA_VISIBLE_DEVICES': '0,1,2,3',           # Use all 4 GPUs\n",
    "        'TORCH_CUDA_ARCH_LIST': '8.9',               # NVIDIA L40S compute capability\n",
    "        \n",
    "        # üóÇÔ∏è Cache Directories\n",
    "        'MODEL_CACHE_DIR': '/opt/ml/model',          # Model cache location\n",
    "        'TRANSFORMERS_CACHE': '/tmp/transformers_cache',  # HuggingFace cache\n",
    "        'HF_HOME': '/tmp/hf_home',                   # HuggingFace home directory\n",
    "        \n",
    "        # üåê Server Configuration\n",
    "        'SAGEMAKER_BIND_TO_PORT': '8080',            # Internal server port\n",
    "        'SAGEMAKER_BIND_TO_HOST': '0.0.0.0',         # Bind to all interfaces\n",
    "        \n",
    "        # üîß Optimization Settings\n",
    "        'NCCL_DEBUG': 'INFO',                        # NCCL debugging (for multi-GPU)\n",
    "        'TORCH_COMPILE_DISABLE': '1',                # Disable PyTorch compilation\n",
    "        'VLLM_DISABLE_CUSTOM_ALL_REDUCE': '1',       # Disable custom all-reduce (stability)\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ SageMaker model created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Deploy Model to SageMaker Endpoint\n",
    "\n",
    "This step creates and deploys the model to a real-time inference endpoint. The deployment includes:\n",
    "\n",
    "- **Model Loading**: Downloading Qwen3-Next-80B-A3B-Instruct from HuggingFace\n",
    "- **vLLM Initialization**: Setting up the inference engine with tensor parallelism\n",
    "- **Health Checks**: Ensuring the endpoint is ready for inference\n",
    "\n",
    "**‚è±Ô∏è Expected Time**: 10-15 minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T16:29:12.197342Z",
     "iopub.status.busy": "2025-09-18T16:29:12.196644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting deployment to endpoint: qwen3-next-80b-a3b-instruct-1758212940\n",
      "‚è±Ô∏è  Estimated Time: 10-15 minutes\n",
      "-"
     ]
    }
   ],
   "source": [
    "# Deploy the model to a SageMaker endpoint\n",
    "print(f\"üöÄ Starting deployment to endpoint: {endpoint_name}\")\n",
    "print(f\"‚è±Ô∏è  Estimated Time: 10-15 minutes\")\n",
    "\n",
    "# Start deployment with optimized settings\n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer(),\n",
    "    container_startup_health_check_timeout=1200,  # 20 minutes for container startup\n",
    "    model_data_download_timeout=1200,             # 20 minutes for model download\n",
    "    wait=True,          # Wait for deployment to complete\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Deployment completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predictor for inference (alternative to deployed predictor)\n",
    "\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "from sagemaker.deserializers import JSONDeserializer\n",
    "\n",
    "predictor = Predictor(\n",
    "    endpoint_name=endpoint_name,\n",
    "    serializer=JSONSerializer(),\n",
    "    deserializer=JSONDeserializer()\n",
    ")\n",
    "\n",
    "print(f\"üîó Connected to endpoint: {endpoint_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Test the Deployed Model\n",
    "\n",
    "Now let's test our deployed Qwen3-Next model with various inference scenarios:\n",
    "\n",
    "### üìù Basic Chat Completion\n",
    "\n",
    "The model supports OpenAI-compatible chat completion format with system and user messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:20:11.308157Z",
     "iopub.status.busy": "2025-09-18T15:20:11.307734Z",
     "iopub.status.idle": "2025-09-18T15:20:22.894193Z",
     "shell.execute_reply": "2025-09-18T15:20:22.893792Z",
     "shell.execute_reply.started": "2025-09-18T15:20:11.308140Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Code Generation Test Completed\n",
      "   Response Time: 11.58 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Generated Code:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here's a Python function to calculate the Fibonacci sequence using dynamic programming with memoization:\n",
       "\n",
       "```python\n",
       "def fibonacci_dp(n, memo={}):\n",
       "    \"\"\"\n",
       "    Calculate the nth Fibonacci number using dynamic programming with memoization.\n",
       "    \n",
       "    Args:\n",
       "        n (int): The position in the Fibonacci sequence (0-indexed)\n",
       "        memo (dict): Dictionary to store previously calculated values (default: {})\n",
       "    \n",
       "    Returns:\n",
       "        int: The nth Fibonacci number\n",
       "    \n",
       "    Examples:\n",
       "        >>> fibonacci_dp(0)\n",
       "        0\n",
       "        >>> fibonacci_dp(1)\n",
       "        1\n",
       "        >>> fibonacci_dp(10)\n",
       "        55\n",
       "        >>> fibonacci_dp(20)\n",
       "        6765\n",
       "    \"\"\"\n",
       "    # Base cases\n",
       "    if n in memo:\n",
       "        return memo[n]\n",
       "    if n <= 1:\n",
       "        return n\n",
       "    \n",
       "    # Calculate using dynamic programming (memoization)\n",
       "    memo[n] = fibonacci_dp(n - 1, memo) + fibonacci_dp(n - 2, memo)\n",
       "    return memo[n]\n",
       "\n",
       "\n",
       "# Alternative iterative approach (more memory efficient)\n",
       "def fibonacci_dp_iterative(n):\n",
       "    \"\"\"\n",
       "    Calculate the nth Fibonacci number using an iterative dynamic programming approach.\n",
       "    This approach uses O(1) space instead of O(n) space for memoization.\n",
       "    \n",
       "    Args:\n",
       "        n (int): The position in the Fibonacci sequence (0-indexed)\n",
       "    \n",
       "    Returns:\n",
       "        int: The nth Fibonacci number\n",
       "    \n",
       "    Examples:\n",
       "        >>> fibonacci_dp_iterative(0)\n",
       "        0\n",
       "        >>> fibonacci_dp_iterative(1)\n",
       "        1\n",
       "        >>> fibonacci_dp_iterative(10)\n",
       "        55\n",
       "        >>> fibonacci_dp_iterative(20)\n",
       "        6765\n",
       "    \"\"\"\n",
       "    if n <= 1:\n",
       "        return n\n",
       "    \n",
       "    # Only keep track of the last two values\n",
       "    prev2 = 0  # F(0)\n",
       "    prev1 = 1  # F(1)\n",
       "    \n",
       "    # Calculate iteratively from 2 to n\n",
       "    for i in range(2, n + 1):\n",
       "        current = prev1 + prev2\n",
       "        prev2 = prev1\n",
       "        prev1 = current\n",
       "    \n",
       "    return prev1\n",
       "\n",
       "\n",
       "# Example usage and test function\n",
       "if __name__ == \"__main__\":\n",
       "    # Test both implementations\n",
       "    print(\"Testing Fibonacci Dynamic Programming implementations:\")\n",
       "    \n",
       "    # Test cases\n",
       "    test_cases = [0, 1, 2, 3, 4, 5, 10, 15, 20]\n",
       "    \n",
       "    print(\"\\nRecursive DP with memoization:\")\n",
       "    for n in test_cases:\n",
       "        result = fibonacci_dp(n)\n",
       "        print(f\"F({n}) = {result}\")\n",
       "    \n",
       "    print(\"\\nIterative DP (space efficient):\")\n",
       "    for n in test_cases:\n",
       "        result = fibonacci_dp_iterative(n)\n",
       "        print(f\"F({n}) = {result}\")\n",
       "    \n",
       "    # Performance comparison for larger values\n",
       "    import time\n",
       "    \n",
       "    n = 35\n",
       "    print(f\"\\nPerformance test for F({n}):\")\n",
       "    \n",
       "    # Test recursive DP\n",
       "    start_time = time.time()\n",
       "    result1 = fibonacci_dp(n)\n",
       "    end_time = time.time()\n",
       "    print(f\"Recursive DP: {result1} (Time: {end_time - start_time:.6f} seconds)\")\n",
       "    \n",
       "    # Test iterative DP\n",
       "    start_time = time.time()\n",
       "    result2 = fibonacci_dp_iterative(n)\n",
       "    end_time = time.time()\n",
       "    print(f\"Iterative DP: {result2} (Time: {end_time - start_time:.6f} seconds)\")\n",
       "```\n",
       "\n",
       "## Key Features:\n",
       "\n",
       "### 1. **Recursive DP with Memoization** (`fibonacci_dp`)\n",
       "- Uses a dictionary to store previously computed values\n",
       "- Avoids redundant calculations by storing results\n",
       "- Time complexity: O(n)\n",
       "- Space complexity: O(n)\n",
       "\n",
       "### 2. **Iterative DP** (`fibonacci_dp_iterative`)\n",
       "- Uses only two variables to track the previous two Fibonacci numbers\n",
       "- More memory efficient - O(1) space complexity\n",
       "- Time complexity: O(n)\n",
       "- No recursion overhead\n",
       "\n",
       "### 3. **Benefits of Dynamic Programming**:\n",
       "- Eliminates exponential time complexity of naive recursion\n",
       "- Each Fibonacci number is calculated only once\n",
       "- Much faster for large values of n\n",
       "\n",
       "### 4. **Usage Notes**:\n",
       "- The recursive version uses a default mutable argument (`memo={}`) which is efficient but can cause issues if used in multi-threaded environments\n",
       "- The iterative version is generally preferred for production code due to its space efficiency and lack of recursion depth limits\n",
       "\n",
       "Both implementations correctly handle edge cases (n=0, n=1) and provide the standard Fibonacci sequence: 0, 1, 1, 2, 3, 5, 8, 13"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üß™ Test 1: Code Generation\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "chat_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful Python programming assistant. Write clean, well-commented code.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Write a Python function to calculate the Fibonacci sequence using dynamic programming.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1000,\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 20\n",
    "}\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "response = predictor.predict(chat_request)\n",
    "end_time = time.time()\n",
    "\n",
    "# Display results\n",
    "print(f\"‚úÖ Code Generation Test Completed\")\n",
    "print(f\"   Response Time: {end_time - start_time:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Render the response as Markdown\n",
    "display(Markdown(\"**Generated Code:**\"))\n",
    "display(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:20:28.453704Z",
     "iopub.status.busy": "2025-09-18T15:20:28.453306Z",
     "iopub.status.idle": "2025-09-18T15:20:39.831938Z",
     "shell.execute_reply": "2025-09-18T15:20:39.831496Z",
     "shell.execute_reply.started": "2025-09-18T15:20:28.453688Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üß™ Testing Scientific Explanation...\n",
      "‚úÖ Scientific Explanation Test Completed\n",
      "   Response Time: 11.37 seconds\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "**Quantum Computing Explanation:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Absolutely! Let‚Äôs break down **quantum computing** in simple, everyday terms ‚Äî no PhD required.\n",
       "\n",
       "---\n",
       "\n",
       "### üåü What is Quantum Computing? (The Simple Version)\n",
       "\n",
       "Imagine a regular computer (like your laptop or phone) uses **bits** ‚Äî tiny switches that are either **0 or 1**. Like a light bulb: off (0) or on (1).\n",
       "\n",
       "A **quantum computer** uses **qubits** (quantum bits). These aren‚Äôt just on or off ‚Äî they can be **both 0 and 1 at the same time**! ü§Ø\n",
       "\n",
       "This strange ability is called **superposition**.\n",
       "\n",
       "---\n",
       "\n",
       "### üîë Three Core Quantum Principles (Explained Simply)\n",
       "\n",
       "#### 1. **Superposition** ‚Äî Being in Two States at Once\n",
       "Think of a spinning coin. While it‚Äôs spinning, it‚Äôs neither purely heads nor tails ‚Äî it‚Äôs kind of both. Only when you stop it (measure it) does it ‚Äúchoose‚Äù one.\n",
       "\n",
       "> ‚úÖ A qubit is like a spinning coin ‚Äî it holds many possibilities until you look.\n",
       "\n",
       "#### 2. **Entanglement** ‚Äî Spooky Connection at a Distance\n",
       "Imagine two magic dice. You roll one in New York, and instantly the other in Tokyo shows the same number ‚Äî even if they‚Äôve never met!\n",
       "\n",
       "> ‚úÖ When two qubits are entangled, changing one instantly affects the other ‚Äî no matter how far apart they are. This lets quantum computers link information in powerful ways.\n",
       "\n",
       "#### 3. **Interference** ‚Äî Amplifying the Right Answers\n",
       "Quantum computers don‚Äôt just guess ‚Äî they use wave-like interference (like sound waves canceling or boosting each other) to **amplify correct answers** and **cancel out wrong ones**.\n",
       "\n",
       "> ‚úÖ It‚Äôs like tuning a radio: you turn the dial until the music gets loud and clear ‚Äî the quantum computer does this mathematically to find the best solution.\n",
       "\n",
       "---\n",
       "\n",
       "### üí° Practical Applications (Where Quantum Computing Will Help)\n",
       "\n",
       "| Area | How Quantum Helps | Real-World Example |\n",
       "|------|-------------------|---------------------|\n",
       "| **Drug Discovery** | Simulates molecules at quantum level | Finding new medicines faster ‚Äî e.g., a cure for Alzheimer‚Äôs or personalized cancer treatments |\n",
       "| **Cybersecurity** | Can break current encryption‚Ä¶ but also create unbreakable encryption | Banks and governments will use **quantum-safe encryption** to protect data |\n",
       "| **Logistics & Optimization** | Finds the best route among billions of options | Amazon could find the fastest delivery routes for 1 million packages in seconds |\n",
       "| **Artificial Intelligence** | Speeds up machine learning training | AI models that learn from less data, recognize patterns faster (e.g., medical imaging) |\n",
       "| **Climate Modeling** | Simulates complex weather and chemical reactions | Better predictions for climate change and clean energy solutions (like better batteries) |\n",
       "| **Finance** | Optimizes portfolios and detects fraud | Hedge funds use it to predict market trends or manage risk better |\n",
       "\n",
       "---\n",
       "\n",
       "### üö´ What Quantum Computers Won‚Äôt Do (Myths Busted)\n",
       "\n",
       "- ‚ùå They won‚Äôt replace your laptop for browsing or Netflix.\n",
       "- ‚ùå They‚Äôre not ‚Äúfaster computers‚Äù for everything ‚Äî only for *very specific* problems.\n",
       "- ‚úÖ They‚Äôre like a **specialized super-tool** ‚Äî great for complex math, chemistry, and optimization.\n",
       "\n",
       "---\n",
       "\n",
       "### üß© Analogy: Quantum vs Classical Computer\n",
       "\n",
       "Think of finding your way out of a maze:\n",
       "\n",
       "- **Classical computer**: Tries one path at a time ‚Äî left, then right, then back, etc. Very slow if the maze is huge.\n",
       "- **Quantum computer**: Explores *all paths at once* using superposition, then uses interference to ‚Äúcollapse‚Äù into the correct exit.\n",
       "\n",
       "---\n",
       "\n",
       "### üåê Real-World Progress (2024)\n",
       "\n",
       "- **IBM**, **Google**, and **Rigetti** have built quantum computers with 100‚Äì1000+ qubits.\n",
       "- **China** and the **US** are investing billions ‚Äî it‚Äôs the new ‚Äúspace race.‚Äù\n",
       "- Companies like **JPMorgan Chase** and **BMW** are already testing quantum algorithms.\n",
       "\n",
       "---\n",
       "\n",
       "### ‚úÖ In a Nutshell\n",
       "\n",
       "> **Quantum computing = Using the weird rules of tiny particles to solve problems too complex for regular computers.**\n",
       "\n",
       "It‚Äôs not magic ‚Äî it‚Äôs physics. And while we‚Äôre still early in the journey, quantum computing could revolutionize medicine, security, AI, and more in the next 10‚Äì20 years.\n",
       "\n",
       "Think of it as the next leap after the transistor ‚Äî and we‚Äôre just turning it on. üîå‚öõÔ∏è\n",
       "\n",
       "Let me know if you‚Äôd like a diagram or a fun analogy using pizza or cats! üòä"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# üß™ Test 2: Scientific Explanation  \n",
    "print(\"üß™ Testing Scientific Explanation...\")\n",
    "\n",
    "science_request = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"You are a helpful science educator. Explain complex topics clearly with examples.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": \"Explain quantum computing principles in simple terms with practical applications.\"\n",
    "        }\n",
    "    ],\n",
    "    \"max_tokens\": 1500,      \n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.8,\n",
    "    \"top_k\": 20\n",
    "}\n",
    "\n",
    "# Measure inference time\n",
    "start_time = time.time()\n",
    "response = predictor.predict(science_request)\n",
    "end_time = time.time()\n",
    "\n",
    "# Display results\n",
    "print(f\"‚úÖ Scientific Explanation Test Completed\")\n",
    "print(f\"   Response Time: {end_time - start_time:.2f} seconds\")\n",
    "print()\n",
    "\n",
    "# Render the response\n",
    "display(Markdown(\"**Quantum Computing Explanation:**\"))\n",
    "display(Markdown(response[\"choices\"][0][\"message\"][\"content\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü§ñ Advanced Integration: Strands Agents with Tool Use\n",
    "\n",
    "Qwen3-Next excels at tool calling and agent workflows. Here we integrate with **Strands Agents** framework for advanced AI agent capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-18T15:22:26.234176Z",
     "iopub.status.busy": "2025-09-18T15:22:26.233731Z",
     "iopub.status.idle": "2025-09-18T15:22:26.300019Z",
     "shell.execute_reply": "2025-09-18T15:22:26.299483Z",
     "shell.execute_reply.started": "2025-09-18T15:22:26.234159Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Strands Agent configured successfully!\n"
     ]
    }
   ],
   "source": [
    "# ü§ñ Configure Strands Agent with SageMaker Integration\n",
    "from strands import Agent\n",
    "from strands.models.sagemaker import SageMakerAIModel\n",
    "from strands_tools import calculator, current_time, file_read, shell\n",
    "\n",
    "\n",
    "# Create SageMaker AI Model for Strands\n",
    "sagemaker_model = SageMakerAIModel(\n",
    "    endpoint_config={\n",
    "        \"endpoint_name\": endpoint_name,      # Use our deployed endpoint\n",
    "        \"region_name\": region,               # AWS region\n",
    "    },\n",
    "    payload_config={\n",
    "        \"max_tokens\": 1000,                  # Response length limit\n",
    "        \"temperature\": 0.7,                  # Creativity level\n",
    "        \"stream\": False,                    \n",
    "    }\n",
    ")\n",
    "\n",
    "# Create agent with useful tools\n",
    "agent = Agent(\n",
    "    model=sagemaker_model, \n",
    "    tools=[\n",
    "        calculator,    # Mathematical calculations\n",
    "        current_time,  # Get current date/time\n",
    "        file_read,     # Read local files\n",
    "        shell          # Execute shell commands (use with caution)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Strands Agent configured successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# üßÆ Test Agent with Mathematical Problem Solving\n",
    "\n",
    "response = agent(\"what's the square root of 12\")\n",
    "\n",
    "print(f\"‚úÖ Agent Response:\")\n",
    "print(f\"   Stop Reason: {response.stop_reason}\")\n",
    "print(f\"   Cycles: {response.metrics.cycle_count}\")\n",
    "print(f\"   Duration: {sum(response.metrics.cycle_durations):.2f}s\")\n",
    "print()\n",
    "\n",
    "# Display the agent's mathematical reasoning\n",
    "display(Markdown(response.message['content'][0]['text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Cleanup and Cost Management\n",
    "\n",
    "**‚ö†Ô∏è Important**: SageMaker endpoints incur costs while running. Remember to clean up resources when testing is complete.\n",
    "\n",
    "### üí∞ Cost Information:\n",
    "- **ml.g6e.12xlarge**: ~$10.00/hour (varies by region)\n",
    "- **Storage**: S3 charges for model artifacts\n",
    "- **Data Transfer**: Charges for inference requests/responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-09-16T13:33:25.413961Z",
     "iopub.status.idle": "2025-09-16T13:33:25.414120Z",
     "shell.execute_reply": "2025-09-16T13:33:25.414046Z",
     "shell.execute_reply.started": "2025-09-16T13:33:25.414038Z"
    }
   },
   "outputs": [],
   "source": [
    "# üóëÔ∏è Delete Endpoint (UNCOMMENT TO EXECUTE)\n",
    "# ‚ö†Ô∏è WARNING: This will delete your endpoint and stop all billing\n",
    "# Only run this when you're completely done with testing\n",
    "\n",
    "# print(f\"üóëÔ∏è Deleting endpoint: {endpoint_name}\")\n",
    "# predictor.delete_endpoint(delete_endpoint_config=True)\n",
    "# print(f\"‚úÖ Endpoint {endpoint_name} deleted successfully\")\n",
    "\n",
    "print(\"üí° Cleanup Instructions:\")\n",
    "print(\"   1. Uncomment the deletion code above\")\n",
    "print(\"   2. Run the cell to delete the endpoint\")\n",
    "print(\"   3. Verify deletion in AWS Console\")\n",
    "print(\"   4. Check that billing has stopped\")\n",
    "\n",
    "# Show current endpoint status\n",
    "try:\n",
    "    import boto3\n",
    "    sm_client = boto3.client('sagemaker', region_name=region)\n",
    "    endpoint_desc = sm_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "    status = endpoint_desc['EndpointStatus']\n",
    "    print(f\"\\nüìä Current endpoint status: {status}\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n‚ùì Could not check endpoint status: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ‚úÖ What We Accomplished\n",
    "\n",
    "1. **Successfully deployed** Qwen3-Next-80B-A3B-Instruct on SageMaker\n",
    "2. **Configured vLLM** with optimized settings for ml.g6e.12xlarge\n",
    "3. **Tested inference** with multiple scenarios (code generation, explanations)\n",
    "4. **Integrated Strands Agents** for advanced tool calling capabilities\n",
    "\n",
    "### üöÄ Key Features Enabled\n",
    "\n",
    "- **80B Parameter Model** with 3B activation per token (MoE efficiency)\n",
    "- **Multi-Token Prediction** for faster inference (2+ tokens per forward pass)\n",
    "- **Tool Calling** integration with Strands Agents framework\n",
    "- **Tensor Parallelism** across 4 NVIDIA L40S GPUs\n",
    "\n",
    "### üõ†Ô∏è Customization Options\n",
    "\n",
    "- **Modify `serving.properties`**: Adjust vLLM parameters\n",
    "- **Update `model.py`**: Add custom preprocessing/postprocessing\n",
    "- **Instance Types**: Scale up to ml.g6e.24xlarge for higher throughput\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
